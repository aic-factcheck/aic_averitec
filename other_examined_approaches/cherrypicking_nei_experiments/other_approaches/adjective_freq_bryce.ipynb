{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use common adjs to find pattern in label *ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               claim    label  \\\n",
      "0  In a letter to Steve Jobs, Sean Connery refuse...  Refuted   \n",
      "1  Trump Administration claimed songwriter Billie...  Refuted   \n",
      "2  Due to Imran Khan's criticism of Macron's comm...  Refuted   \n",
      "3  UNESCO declared Nadar community as the most an...  Refuted   \n",
      "4  Republican Matt Gaetz was part of a company th...  Refuted   \n",
      "\n",
      "   emotional_word_count  \n",
      "0                     0  \n",
      "1                     0  \n",
      "2                     0  \n",
      "3                     1  \n",
      "4                     0  \n",
      "label\n",
      "Conflicting Evidence/Cherrypicking    0.131579\n",
      "Not Enough Evidence                   0.857143\n",
      "Refuted                               0.131148\n",
      "Supported                             0.327869\n",
      "Name: emotional_word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/home/aaronbry/my_env/data/dev.json'\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Define a list of common emotional words\n",
    "#emotional_words = [\"love\", \"hate\", \"wonderful\", \"excess\", \"terrible\", \"fantastic\", \"awful\", \"great\", \"bad\", \"amazing\", \"horrible\", \"excellent\", \"poor\", \"disaster\"]\n",
    "emotional_words = [\n",
    "    'more',\n",
    "    'most',\n",
    "    'higher',\n",
    "    'biggest',\n",
    "    'best',\n",
    "    'cheaper',\n",
    "    'lower',\n",
    "    'fewer',\n",
    "    'less',\n",
    "    'nigeria',\n",
    "    'bigger',\n",
    "    'greatest',\n",
    "    'speaker',\n",
    "    'worse',\n",
    "    'older',\n",
    "    'least',\n",
    "    'saudi',\n",
    "    'lighter',\n",
    "    'fastest',\n",
    "    'wildfire',\n",
    "    'greater',\n",
    "    'highest',\n",
    "    'richest'\n",
    "]\n",
    "\n",
    "# Preprocess the text to make it easier to count the emotional words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Function to count the number of emotional words in each claim\n",
    "def count_emotional_words(tokens, emotional_words):\n",
    "    count = sum(1 for word in tokens if word in emotional_words)\n",
    "    return count\n",
    "\n",
    "# Apply preprocessing and count emotional words\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "data['emotional_word_count'] = data['tokens'].apply(lambda tokens: count_emotional_words(tokens, emotional_words))\n",
    "\n",
    "# Display the DataFrame with the new columns\n",
    "print(data[['claim', 'label', 'emotional_word_count']].head())\n",
    "\n",
    "# Analyze the correlation between emotional word count and the labels\n",
    "emotional_word_count_analysis = data.groupby('label')['emotional_word_count'].mean()\n",
    "print(emotional_word_count_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2 *ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               claim    label  \\\n",
      "0  In a letter to Steve Jobs, Sean Connery refuse...  Refuted   \n",
      "1  Trump Administration claimed songwriter Billie...  Refuted   \n",
      "2  Due to Imran Khan's criticism of Macron's comm...  Refuted   \n",
      "3  UNESCO declared Nadar community as the most an...  Refuted   \n",
      "4  Republican Matt Gaetz was part of a company th...  Refuted   \n",
      "\n",
      "   emotional_word_count  \n",
      "0                     0  \n",
      "1                     0  \n",
      "2                     0  \n",
      "3                     1  \n",
      "4                     0  \n",
      "label\n",
      "Conflicting Evidence/Cherrypicking    0.131579\n",
      "Not Enough Evidence                   0.857143\n",
      "Refuted                               0.131148\n",
      "Supported                             0.327869\n",
      "Name: emotional_word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/home/aaronbry/my_env/data/dev.json'\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Define the list of common emotional words\n",
    "emotional_words_list = [\n",
    "    'more', 'most', 'higher', 'biggest', 'best', 'cheaper', 'lower', 'fewer',\n",
    "    'less', 'nigeria', 'bigger', 'greatest', 'speaker', 'worse', 'older',\n",
    "    'least', 'saudi', 'lighter', 'fastest', 'wildfire', 'greater', 'highest',\n",
    "    'richest'\n",
    "]\n",
    "\n",
    "# Preprocess the text to make it easier to count the emotional words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Function to count the number of emotional words in each claim\n",
    "def count_emotional_words(tokens, emotional_words):\n",
    "    count = sum(1 for word in tokens if word in emotional_words)\n",
    "    return count\n",
    "\n",
    "# Apply preprocessing and count emotional words\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "data['emotional_word_count'] = data['tokens'].apply(lambda tokens: count_emotional_words(tokens, emotional_words_list))\n",
    "\n",
    "# Display the DataFrame with the new columns\n",
    "print(data[['claim', 'label', 'emotional_word_count']].head())\n",
    "\n",
    "# Analyze the correlation between emotional word count and the labels\n",
    "emotional_word_count_analysis = data.groupby('label')['emotional_word_count'].mean()\n",
    "print(emotional_word_count_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Adjective / Adverb frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word  Frequency\n",
      "0       more         11\n",
      "1       most          8\n",
      "2     higher          5\n",
      "3    biggest          4\n",
      "4       best          3\n",
      "5    cheaper          3\n",
      "6      lower          3\n",
      "7      fewer          2\n",
      "8       less          2\n",
      "9    nigeria          2\n",
      "10    bigger          1\n",
      "11  greatest          1\n",
      "12   speaker          1\n",
      "13     worse          1\n",
      "14     older          1\n",
      "15     least          1\n",
      "16     saudi          1\n",
      "17   lighter          1\n",
      "18   fastest          1\n",
      "19  wildfire          1\n",
      "20   greater          1\n",
      "21   highest          1\n",
      "22   richest          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "#nltk.download('punkt')\n",
    "# Download necessary NLTK data\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/home/aaronbry/my_env/data/dev.json'\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Preprocess the text to make it easier to extract words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "\n",
    "# Flatten all tokens into a single list\n",
    "all_tokens = [token for tokens in data['tokens'] for token in tokens]\n",
    "\n",
    "# POS tagging to filter adjectives and emotional words\n",
    "tagged_tokens = nltk.pos_tag(all_tokens)\n",
    "adjective_words = [word for word, pos in tagged_tokens if pos in ['JJR', 'JJS', 'RB' 'RBR', 'RBS']]\n",
    "\n",
    "# Frequency distribution of adjectives/emotional words\n",
    "freq_dist = Counter(adjective_words)\n",
    "\n",
    "# Get the 100 most common adjectives/emotional words\n",
    "most_common_adjectives = freq_dist.most_common(100)\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "most_common_adjectives_df = pd.DataFrame(most_common_adjectives, columns=['Word', 'Frequency'])\n",
    "print(most_common_adjectives_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use frequency of POS to identify most common words (adjs, adv, modifiers) in claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word  Frequency\n",
      "0       more        110\n",
      "1       less         27\n",
      "2      lower         18\n",
      "3     higher         16\n",
      "4      fewer         11\n",
      "5     better          8\n",
      "6     longer          3\n",
      "7      older          3\n",
      "8    younger          3\n",
      "9    smaller          3\n",
      "10   greater          2\n",
      "11      lakh          2\n",
      "12  offender          1\n",
      "13    cooper          1\n",
      "14   cheaper          1\n",
      "15    easier          1\n",
      "16   tougher          1\n",
      "17   soldier          1\n",
      "18      paid          1\n",
      "19    muslim          1\n",
      "20    bigger          1\n",
      "21     crore          1\n",
      "22     roger          1\n",
      "                                                claim  \\\n",
      "0   Hunter Biden had no experience in Ukraine or i...   \n",
      "1   Donald Trump delivered the largest tax cuts in...   \n",
      "2   In Nigeria … in terms of revenue share, 20% go...   \n",
      "3   Biden has pledged to stop border wall construc...   \n",
      "4   After the police shooting of Jacob Blake, Gov....   \n",
      "5   The Common Law Admission Test (CLAT) 2020 will...   \n",
      "6         35% of revenue goes to states. (In Nigeria)   \n",
      "7   Margaret Sanger was a racist who believed in e...   \n",
      "8   Joe Biden voted for the Irag War and supported...   \n",
      "9          Biden will take away the Second Amendment.   \n",
      "10             Biden has pledged to defund the police   \n",
      "11  45% of Nigeria's revenue goes to the federal g...   \n",
      "12  Donald Trump inherited a stagnant economy and ...   \n",
      "13  Joe Biden voted for the Iraq War and he suppor...   \n",
      "14  President Muhammadu Buhari sign a new Police b...   \n",
      "15  Plant Seeds Can Restructure Their Own DNA Thro...   \n",
      "16  We actually saw revenues to the Treasury incre...   \n",
      "17  In 2019 the US Police were 20.8 times more lik...   \n",
      "18  A greater proportion of 16 year olds vote in e...   \n",
      "19       Donald Trump has kept his promises to voters   \n",
      "\n",
      "                                 label  common_adjective_count  \n",
      "0                            Supported                       0  \n",
      "1                              Refuted                       0  \n",
      "2                            Supported                       0  \n",
      "3                            Supported                       0  \n",
      "4                              Refuted                       0  \n",
      "5                              Refuted                       0  \n",
      "6                              Refuted                       0  \n",
      "7   Conflicting Evidence/Cherrypicking                       0  \n",
      "8   Conflicting Evidence/Cherrypicking                       0  \n",
      "9                              Refuted                       0  \n",
      "10                             Refuted                       0  \n",
      "11                             Refuted                       0  \n",
      "12  Conflicting Evidence/Cherrypicking                       0  \n",
      "13                           Supported                       0  \n",
      "14                           Supported                       0  \n",
      "15                             Refuted                       0  \n",
      "16                             Refuted                       0  \n",
      "17                           Supported                       1  \n",
      "18  Conflicting Evidence/Cherrypicking                       1  \n",
      "19  Conflicting Evidence/Cherrypicking                       0  \n",
      "['JJR']\n",
      "label\n",
      "Conflicting Evidence/Cherrypicking    0.158974\n",
      "Not Enough Evidence                   0.141844\n",
      "Refuted                               0.078645\n",
      "Supported                             0.176678\n",
      "Name: common_adjective_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'my_env/data/train.json'  # Change to your path\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Preprocess the text to make it easier to extract words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "\n",
    "# Flatten all tokens into a single list\n",
    "all_tokens = [token for tokens in data['tokens'] for token in tokens]\n",
    "\n",
    "# POS tagging to filter adjectives and emotional words\n",
    "tagged_tokens = nltk.pos_tag(all_tokens)\n",
    "selected_pos = ['JJR']\n",
    "#['JJR', 'JJS', 'RB', 'RBR', 'RBS', 'MD', 'UH']\n",
    "adjective_words = [word for word, pos in tagged_tokens if pos in selected_pos]\n",
    "\n",
    "# Frequency distribution of adjectives/emotional words\n",
    "freq_dist = Counter(adjective_words)\n",
    "\n",
    "# Get the 100 most common adjectives/emotional words\n",
    "most_common_adjectives = freq_dist.most_common(50)\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "most_common_adjectives_df = pd.DataFrame(most_common_adjectives, columns=['Word', 'Frequency'])\n",
    "print(most_common_adjectives_df[:25])\n",
    "\n",
    "# Create a set of the most common adjectives for quick lookup\n",
    "most_common_adjectives_set = set([word for word, _ in most_common_adjectives])\n",
    "\n",
    "# Function to count the number of most common adjectives in each claim\n",
    "def count_common_adjectives(tokens, common_adjectives):\n",
    "    count = sum(1 for word in tokens if word in common_adjectives)\n",
    "    return count\n",
    "\n",
    "# Apply preprocessing and count common adjectives\n",
    "data['common_adjective_count'] = data['tokens'].apply(lambda tokens: count_common_adjectives(tokens, most_common_adjectives_set))\n",
    "\n",
    "# Display the DataFrame with the new columns\n",
    "print(data[['claim', 'label', 'common_adjective_count']].head(20))\n",
    "\n",
    "# Analyze the correlation between common adjective count and the labels\n",
    "common_adjective_count_analysis = data.groupby('label')['common_adjective_count'].mean()\n",
    "print(selected_pos)\n",
    "print(common_adjective_count_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Most Common POS Tags:\n",
      "[('NN', 2997), ('IN', 2695), ('DT', 2307), ('JJ', 2307), ('NNS', 2281), ('VBD', 1296), ('VBN', 1284), ('VBZ', 1251), ('VB', 1072), ('TO', 1002)]\n",
      "\n",
      "Top 5 POS Tags for 'Conflicting Evidence/Cherrypicking':\n",
      "[('NN', 765), ('IN', 381), ('NNS', 295), ('JJ', 277), ('DT', 256)]\n",
      "\n",
      "Top 5 POS Tags for 'Not Enough Evidence':\n",
      "[('NN', 1188), ('IN', 724), ('NNS', 439), ('JJ', 434), ('DT', 425)]\n",
      "\n",
      "Top 5 POS Tags for 'Refuted':\n",
      "[('NN', 7578), ('IN', 3774), ('JJ', 2690), ('DT', 2615), ('NNS', 2357)]\n",
      "\n",
      "Top 5 POS Tags for 'Supported':\n",
      "[('NN', 3407), ('IN', 2066), ('JJ', 1372), ('DT', 1236), ('NNS', 1236)]\n",
      "\n",
      "Top 5 POS Tags for 'Conflicting Evidence/Cherrypicking':\n",
      "NN: 765\n",
      "IN: 381\n",
      "NNS: 295\n",
      "JJ: 277\n",
      "DT: 256\n",
      "\n",
      "Top 5 POS Tags for 'Not Enough Evidence':\n",
      "NN: 1188\n",
      "IN: 724\n",
      "NNS: 439\n",
      "JJ: 434\n",
      "DT: 425\n",
      "\n",
      "Top 5 POS Tags for 'Refuted':\n",
      "NN: 7578\n",
      "IN: 3774\n",
      "JJ: 2690\n",
      "DT: 2615\n",
      "NNS: 2357\n",
      "\n",
      "Top 5 POS Tags for 'Supported':\n",
      "NN: 3407\n",
      "IN: 2066\n",
      "JJ: 1372\n",
      "DT: 1236\n",
      "NNS: 1236\n",
      "\n",
      "Mean number of each POS tag per label:\n",
      "JJ      1193.25\n",
      "NN      3234.50\n",
      "VBD      443.00\n",
      "DT      1133.00\n",
      "WP        38.75\n",
      "IN      1736.25\n",
      "NNS     1081.75\n",
      "PRP$     103.50\n",
      "WRB       27.00\n",
      "TO       314.25\n",
      "VB       365.00\n",
      "CC       254.25\n",
      "RB       305.50\n",
      "PRP      238.75\n",
      "JJR       52.00\n",
      "VBP      298.00\n",
      "VBZ      368.75\n",
      "VBN      420.25\n",
      "CD       171.50\n",
      "VBG      271.25\n",
      "MD       116.00\n",
      "RP        39.50\n",
      "WDT       44.25\n",
      "RBR       18.75\n",
      "EX        23.75\n",
      "JJS       29.50\n",
      "RBS        6.50\n",
      "NNP        9.25\n",
      "PDT        8.75\n",
      "FW         1.75\n",
      "NNPS       2.25\n",
      "WP$        0.25\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'my_env/data/train.json'  # Change to your path\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Preprocess the text to make it easier to extract words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "\n",
    "# Function to extract POS tags\n",
    "def extract_pos_tags(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "data['pos_tags'] = data['tokens'].apply(extract_pos_tags)\n",
    "\n",
    "# Function to count POS tags in each claim\n",
    "def count_pos_tags(pos_tags):\n",
    "    pos_counts = Counter(pos for word, pos in pos_tags)\n",
    "    return pos_counts\n",
    "\n",
    "# Apply function to count POS tags\n",
    "data['pos_counts'] = data['pos_tags'].apply(count_pos_tags)\n",
    "\n",
    "# Flatten all POS tags into a single list for overall frequency\n",
    "all_pos_tags = [pos for pos_counts in data['pos_counts'] for pos in pos_counts]\n",
    "overall_pos_freq = Counter(all_pos_tags)\n",
    "\n",
    "# Display the overall most common POS tags\n",
    "print(\"Overall Most Common POS Tags:\")\n",
    "print(overall_pos_freq.most_common(10))\n",
    "\n",
    "# Aggregate results by label\n",
    "def sum_counters(counters):\n",
    "    sum_counter = Counter()\n",
    "    for counter in counters:\n",
    "        if isinstance(counter, Counter):  # Ensure that we only update with Counter objects\n",
    "            sum_counter.update(counter)\n",
    "    return sum_counter\n",
    "\n",
    "# Apply sum_counters function to ensure proper aggregation\n",
    "label_pos_counts = data.groupby('label')['pos_counts'].apply(lambda x: sum_counters(x))\n",
    "\n",
    "# Ensure each entry in label_pos_counts is a Counter object\n",
    "label_pos_counts = {label: sum_counters(group) for label, group in data.groupby('label')['pos_counts']}\n",
    "\n",
    "# Display the top 5 POS tags per label\n",
    "for label, pos_counts in label_pos_counts.items():\n",
    "    print(f\"\\nTop 5 POS Tags for '{label}':\")\n",
    "    if isinstance(pos_counts, Counter):\n",
    "        print(pos_counts.most_common(5))\n",
    "    else:\n",
    "        print(\"No POS counts available for this label.\")\n",
    "\n",
    "# Function to get top N POS tags per label\n",
    "def top_n_pos_tags_per_label(data, n=5):\n",
    "    label_pos_counts = {label: sum_counters(group) for label, group in data.groupby('label')['pos_counts']}\n",
    "    top_pos_tags = {label: counts.most_common(n) for label, counts in label_pos_counts.items() if isinstance(counts, Counter)}\n",
    "    return top_pos_tags\n",
    "\n",
    "# Get the top 5 POS tags per label\n",
    "top_pos_tags = top_n_pos_tags_per_label(data, n=5)\n",
    "\n",
    "# Display the top 5 POS tags per label\n",
    "for label, pos_tags in top_pos_tags.items():\n",
    "    print(f\"\\nTop 5 POS Tags for '{label}':\")\n",
    "    for pos, count in pos_tags:\n",
    "        print(f\"{pos}: {count}\")\n",
    "\n",
    "# Additional statistics (e.g., mean number of each POS tag per label)\n",
    "def calculate_mean_pos_per_label(data):\n",
    "    label_pos_counts = {label: sum_counters(group) for label, group in data.groupby('label')['pos_counts']}\n",
    "    pos_tag_stats = pd.DataFrame([dict(counter) for counter in label_pos_counts.values() if isinstance(counter, Counter)]).fillna(0).mean()\n",
    "    return pos_tag_stats\n",
    "\n",
    "pos_tag_stats = calculate_mean_pos_per_label(data)\n",
    "\n",
    "print(\"\\nMean number of each POS tag per label:\")\n",
    "print(pos_tag_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi ^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top POS tags by Chi-Square test:\n",
      "POS Tag: CD, Chi2: 55.67679084161057, p-value: 4.923975397460288e-12\n",
      "POS Tag: VB, Chi2: 26.434160091168728, p-value: 7.735751751849582e-06\n",
      "POS Tag: MD, Chi2: 24.82168425892936, p-value: 1.6824400398383774e-05\n",
      "POS Tag: JJR, Chi2: 24.294370481646954, p-value: 2.1683108341069872e-05\n",
      "POS Tag: NNS, Chi2: 22.43599054320315, p-value: 5.293002923054079e-05\n",
      "POS Tag: IN, Chi2: 15.81278952091885, p-value: 0.0012387295412777974\n",
      "POS Tag: RBR, Chi2: 14.960175648701716, p-value: 0.0018509998096514445\n",
      "POS Tag: WP, Chi2: 10.869356367762885, p-value: 0.012453703243457464\n",
      "POS Tag: CC, Chi2: 9.940817523178055, p-value: 0.01907596552265242\n",
      "POS Tag: VBN, Chi2: 9.706298818815164, p-value: 0.021234996947359988\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "# Create a contingency table\n",
    "def create_contingency_table(data, pos_tag):\n",
    "    table = pd.crosstab(data['label'], data['pos_counts'].apply(lambda x: x.get(pos_tag, 0) > 0))\n",
    "    return table\n",
    "\n",
    "# Perform Chi-Square test for each POS tag\n",
    "chi2_results = {}\n",
    "for pos_tag in overall_pos_freq.keys():\n",
    "    contingency_table = create_contingency_table(data, pos_tag)\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    chi2_results[pos_tag] = (chi2, p)\n",
    "\n",
    "# Get the POS tags with the lowest p-values\n",
    "sorted_chi2_results = sorted(chi2_results.items(), key=lambda item: item[1][1])\n",
    "print(\"Top POS tags by Chi-Square test:\")\n",
    "for pos_tag, (chi2, p) in sorted_chi2_results[:10]:\n",
    "    print(f\"POS Tag: {pos_tag}, Chi2: {chi2}, p-value: {p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top POS tags by Mutual Information:\n",
      "CD     0.011774\n",
      "IN     0.011712\n",
      "NN     0.010345\n",
      "NNS    0.009498\n",
      "VB     0.008839\n",
      "CC     0.006807\n",
      "DT     0.006082\n",
      "MD     0.006080\n",
      "PRP    0.005993\n",
      "JJ     0.005534\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Create a DataFrame where each column is a POS tag and each row is a claim\n",
    "pos_tag_df = pd.DataFrame([{pos: count for pos, count in counter.items()} for counter in data['pos_counts']]).fillna(0)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = {label: idx for idx, label in enumerate(data['label'].unique())}\n",
    "y = data['label'].map(label_encoder)\n",
    "\n",
    "# Calculate mutual information\n",
    "mi = mutual_info_classif(pos_tag_df, y, discrete_features=True)\n",
    "\n",
    "# Get the top POS tags by mutual information\n",
    "mi_scores = pd.Series(mi, index=pos_tag_df.columns)\n",
    "mi_scores = mi_scores.sort_values(ascending=False)\n",
    "print(\"Top POS tags by Mutual Information:\")\n",
    "print(mi_scores.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier for label: Conflicting Evidence/Cherrypicking\n",
      "Training classifier for label: Not Enough Evidence\n",
      "Training classifier for label: Refuted\n",
      "Training classifier for label: Supported\n",
      "\n",
      "Top 5 POS tags for label 'Conflicting Evidence/Cherrypicking':\n",
      "JJR     0.415546\n",
      "RP      0.246184\n",
      "NNS     0.217197\n",
      "PRP$    0.206919\n",
      "CC      0.195953\n",
      "dtype: float64\n",
      "\n",
      "Bottom 5 POS tags for label 'Conflicting Evidence/Cherrypicking':\n",
      "VBN   -0.360008\n",
      "WRB   -0.390771\n",
      "EX    -0.557441\n",
      "PDT   -0.748603\n",
      "JJS   -1.084974\n",
      "dtype: float64\n",
      "\n",
      "Top 5 POS tags for label 'Not Enough Evidence':\n",
      "NNPS    1.254270\n",
      "PDT     0.679914\n",
      "WRB     0.461062\n",
      "RBR     0.401490\n",
      "CC      0.392261\n",
      "dtype: float64\n",
      "\n",
      "Bottom 5 POS tags for label 'Not Enough Evidence':\n",
      "JJ     -0.118838\n",
      "PRP$   -0.199226\n",
      "FW     -0.266749\n",
      "PRP    -0.358378\n",
      "NNP    -0.715390\n",
      "dtype: float64\n",
      "\n",
      "Top 5 POS tags for label 'Refuted':\n",
      "FW     1.218443\n",
      "EX     0.311199\n",
      "WP     0.278311\n",
      "WP$    0.245163\n",
      "RBS    0.227018\n",
      "dtype: float64\n",
      "\n",
      "Bottom 5 POS tags for label 'Refuted':\n",
      "TO    -0.271341\n",
      "WDT   -0.298864\n",
      "CD    -0.357227\n",
      "JJR   -0.432570\n",
      "RBR   -0.620916\n",
      "dtype: float64\n",
      "\n",
      "Top 5 POS tags for label 'Supported':\n",
      "RBR    0.399382\n",
      "JJR    0.359911\n",
      "TO     0.314293\n",
      "NNP    0.290118\n",
      "CD     0.228353\n",
      "dtype: float64\n",
      "\n",
      "Bottom 5 POS tags for label 'Supported':\n",
      "RBS    -0.326874\n",
      "EX     -0.444007\n",
      "WP     -0.446281\n",
      "NNPS   -0.863679\n",
      "FW     -0.953218\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Prepare the dataset\n",
    "X = pos_tag_df\n",
    "y = data['label']\n",
    "\n",
    "# Binarize the labels for one-vs-rest classification\n",
    "lb = LabelBinarizer()\n",
    "y_bin = lb.fit_transform(y)\n",
    "\n",
    "# Function to train a classifier for each label\n",
    "def train_classifier_per_label(X, y_bin, label_names):\n",
    "    feature_importance = {}\n",
    "    for i, label in enumerate(label_names):\n",
    "        print(f\"Training classifier for label: {label}\")\n",
    "        y_label = y_bin[:, i]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_label, test_size=0.2, random_state=42)\n",
    "        \n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Get the coefficients\n",
    "        coef = pd.Series(model.coef_[0], index=X.columns)\n",
    "        feature_importance[label] = coef.sort_values(ascending=False)\n",
    "        \n",
    "    return feature_importance\n",
    "\n",
    "# Train classifiers and get feature importance\n",
    "label_names = lb.classes_\n",
    "feature_importance = train_classifier_per_label(X, y_bin, label_names)\n",
    "\n",
    "# Display the top 5 POS tags by coefficients for each label\n",
    "for label, coef in feature_importance.items():\n",
    "    print(f\"\\nTop 5 POS tags for label '{label}':\")\n",
    "    print(coef.head(5))\n",
    "    print(f\"\\nBottom 5 POS tags for label '{label}':\")\n",
    "    print(coef.tail(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of 'pos_counts':\n",
      "0    {'NN': 8, 'VBD': 2, 'DT': 3, 'IN': 3, 'CC': 1,...\n",
      "1    {'JJ': 2, 'NN': 3, 'VBD': 1, 'DT': 1, 'JJS': 1...\n",
      "2    {'IN': 3, 'NNS': 2, 'NN': 3, 'VBZ': 1, 'TO': 1...\n",
      "3    {'NN': 7, 'VBZ': 1, 'VBN': 1, 'TO': 2, 'VB': 2...\n",
      "4    {'IN': 3, 'DT': 1, 'NN': 4, 'NNS': 3, 'VBP': 2...\n",
      "Name: pos_counts, dtype: object\n",
      "\n",
      "Overall Most Common POS Tags:\n",
      "[('NN', 2997), ('IN', 2695), ('DT', 2307), ('JJ', 2307), ('NNS', 2281), ('VBD', 1296), ('VBN', 1284), ('VBZ', 1251), ('VB', 1072), ('TO', 1002)]\n",
      "\n",
      "Top 5 POS Tags for 'Conflicting Evidence/Cherrypicking':\n",
      "[('NN', 765), ('IN', 381), ('NNS', 295), ('JJ', 277), ('DT', 256)]\n",
      "\n",
      "Top 5 POS Tags for 'Not Enough Evidence':\n",
      "[('NN', 1188), ('IN', 724), ('NNS', 439), ('JJ', 434), ('DT', 425)]\n",
      "\n",
      "Top 5 POS Tags for 'Refuted':\n",
      "[('NN', 7578), ('IN', 3774), ('JJ', 2690), ('DT', 2615), ('NNS', 2357)]\n",
      "\n",
      "Top 5 POS Tags for 'Supported':\n",
      "[('NN', 3407), ('IN', 2066), ('JJ', 1372), ('DT', 1236), ('NNS', 1236)]\n",
      "\n",
      "Classification Report:\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "Conflicting Evidence/Cherrypicking       0.07      0.57      0.13       195\n",
      "               Not Enough Evidence       0.12      0.12      0.12       282\n",
      "                           Refuted       0.64      0.13      0.22      1742\n",
      "                         Supported       0.34      0.36      0.35       849\n",
      "\n",
      "                          accuracy                           0.22      3068\n",
      "                         macro avg       0.29      0.30      0.21      3068\n",
      "                      weighted avg       0.47      0.22      0.24      3068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'my_env/data/train.json'  # Change to your path\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Preprocess the text to make it easier to extract words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "\n",
    "# Function to extract POS tags\n",
    "def extract_pos_tags(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "data['pos_tags'] = data['tokens'].apply(extract_pos_tags)\n",
    "\n",
    "# Function to count POS tags in each claim\n",
    "def count_pos_tags(pos_tags):\n",
    "    pos_counts = Counter(pos for word, pos in pos_tags)\n",
    "    return pos_counts\n",
    "\n",
    "# Apply function to count POS tags\n",
    "data['pos_counts'] = data['pos_tags'].apply(count_pos_tags)\n",
    "\n",
    "# Debugging: Verify that 'pos_counts' column exists and is correctly populated\n",
    "print(\"\\nSample of 'pos_counts':\")\n",
    "print(data['pos_counts'].head())\n",
    "\n",
    "# Flatten all POS tags into a single list for overall frequency\n",
    "all_pos_tags = [pos for pos_counts in data['pos_counts'] for pos in pos_counts]\n",
    "overall_pos_freq = Counter(all_pos_tags)\n",
    "\n",
    "# Display the overall most common POS tags\n",
    "print(\"\\nOverall Most Common POS Tags:\")\n",
    "print(overall_pos_freq.most_common(10))\n",
    "\n",
    "# Aggregate results by label\n",
    "def sum_counters(counters):\n",
    "    sum_counter = Counter()\n",
    "    for counter in counters:\n",
    "        if isinstance(counter, Counter):  # Ensure that we only update with Counter objects\n",
    "            sum_counter.update(counter)\n",
    "    return sum_counter\n",
    "\n",
    "# Apply sum_counters function to ensure proper aggregation\n",
    "label_pos_counts = data.groupby('label')['pos_counts'].apply(lambda x: sum_counters(x))\n",
    "\n",
    "# Ensure each entry in label_pos_counts is a Counter object\n",
    "label_pos_counts = {label: sum_counters(group) for label, group in data.groupby('label')['pos_counts']}\n",
    "\n",
    "# Display the top 5 POS tags per label\n",
    "for label, pos_counts in label_pos_counts.items():\n",
    "    print(f\"\\nTop 5 POS Tags for '{label}':\")\n",
    "    if isinstance(pos_counts, Counter):\n",
    "        print(pos_counts.most_common(5))\n",
    "    else:\n",
    "        print(\"No POS counts available for this label.\")\n",
    "\n",
    "# POS tags to consider based on the results (top 5 and bottom 5 for each label)\n",
    "pos_tags_considered = {\n",
    "    'Conflicting Evidence/Cherrypicking': ['JJR', 'RP', 'NNS', 'PRP$', 'CC', 'VBN', 'WRB', 'EX', 'PDT', 'JJS'],\n",
    "    'Not Enough Evidence': ['NNPS', 'PDT', 'WRB', 'RBR', 'CC', 'JJ', 'PRP$', 'FW', 'PRP', 'NNP'],\n",
    "    'Refuted': ['FW', 'EX', 'WP', 'WP$', 'RBS', 'TO', 'WDT', 'CD', 'JJR', 'RBR'],\n",
    "    'Supported': ['RBR', 'JJR', 'TO', 'NNP', 'CD', 'RBS', 'EX', 'WP', 'NNPS', 'FW']\n",
    "}\n",
    "\n",
    "# Coefficients for each label\n",
    "coefficients = {\n",
    "    'Conflicting Evidence/Cherrypicking': {'JJR': 0.415546, 'RP': 0.246184, 'NNS': 0.217197, 'PRP$': 0.206919, 'CC': 0.195953, 'VBN': -0.360008, 'WRB': -0.390771, 'EX': -0.557441, 'PDT': -0.748603, 'JJS': -1.084974},\n",
    "    'Not Enough Evidence': {'NNPS': 1.254270, 'PDT': 0.679914, 'WRB': 0.461062, 'RBR': 0.401490, 'CC': 0.392261, 'JJ': -0.118838, 'PRP$': -0.199226, 'FW': -0.266749, 'PRP': -0.358378, 'NNP': -0.715390},\n",
    "    'Refuted': {'FW': 1.218443, 'EX': 0.311199, 'WP': 0.278311, 'WP$': 0.245163, 'RBS': 0.227018, 'TO': -0.271341, 'WDT': -0.298864, 'CD': -0.357227, 'JJR': -0.432570, 'RBR': -0.620916},\n",
    "    'Supported': {'RBR': 0.399382, 'JJR': 0.359911, 'TO': 0.314293, 'NNP': 0.290118, 'CD': 0.228353, 'RBS': -0.326874, 'EX': -0.444007, 'WP': -0.446281, 'NNPS': -0.863679, 'FW': -0.953218}\n",
    "}\n",
    "\n",
    "# Function to classify a single claim\n",
    "def classify_claim(pos_counts):\n",
    "    scores = {label: 0.0 for label in coefficients.keys()}\n",
    "    for label in coefficients.keys():\n",
    "        for pos_tag, coef in coefficients[label].items():\n",
    "            scores[label] += coef * pos_counts.get(pos_tag, 0)\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "# Apply classification to each claim\n",
    "data['predicted_label'] = data['pos_counts'].apply(classify_claim)\n",
    "\n",
    "# Evaluate the classification\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(data['label'], data['predicted_label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered POS Tags: ['NN', 'VBD', 'DT', 'IN', 'CC', 'WRB', 'PRP', 'JJ', 'JJS', 'NNS', 'VBZ', 'TO', 'VBN', 'VB', 'VBP', 'RB', 'MD', 'WP', 'PRP$', 'RP', 'RBR', 'JJR', 'EX', 'CD', 'VBG', 'WDT', 'RBS', 'PDT', 'NNPS', 'NNP']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Prepare the dataset\n",
    "X = pos_tag_df\n",
    "y = data['label'].map(label_encoder)\n",
    "\n",
    "# Train a RandomForest classifier\n",
    "model = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "filtered_pos_tags = feature_importances[feature_importances > 0.001].index.tolist()  # Keep only features with importance > 0.01\n",
    "print(f\"Filtered POS Tags: {filtered_pos_tags}\")\n",
    "\n",
    "# Update coefficients and pos_tags_considered with filtered POS tags\n",
    "coefficients_filtered = {label: {pos_tag: coef for pos_tag, coef in coefs.items() if pos_tag in filtered_pos_tags} for label, coefs in coefficients.items()}\n",
    "pos_tags_considered_filtered = {label: [pos_tag for pos_tag in pos_tags if pos_tag in filtered_pos_tags] for label, pos_tags in pos_tags_considered.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Filtered POS Tags):\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "Conflicting Evidence/Cherrypicking       0.07      0.57      0.13       195\n",
      "               Not Enough Evidence       0.12      0.12      0.12       282\n",
      "                           Refuted       0.64      0.13      0.22      1742\n",
      "                         Supported       0.34      0.36      0.35       849\n",
      "\n",
      "                          accuracy                           0.22      3068\n",
      "                         macro avg       0.29      0.30      0.20      3068\n",
      "                      weighted avg       0.47      0.22      0.24      3068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to classify a single claim using filtered POS tags\n",
    "def classify_claim_filtered(pos_counts):\n",
    "    scores = {label: 0.0 for label in coefficients_filtered.keys()}\n",
    "    for label in coefficients_filtered.keys():\n",
    "        for pos_tag, coef in coefficients_filtered[label].items():\n",
    "            scores[label] += coef * pos_counts.get(pos_tag, 0)\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "# Apply classification to each claim\n",
    "data['predicted_label_filtered'] = data['pos_counts'].apply(classify_claim_filtered)\n",
    "\n",
    "# Evaluate the classification\n",
    "print(\"\\nClassification Report (Filtered POS Tags):\")\n",
    "print(classification_report(data['label'], data['predicted_label_filtered']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of 'pos_counts':\n",
      "7     {'JJ': 1, 'NN': 5, 'VBD': 4, 'DT': 1, 'WP': 1,...\n",
      "8     {'NN': 5, 'VBD': 2, 'IN': 2, 'DT': 1, 'JJ': 2,...\n",
      "12    {'JJ': 2, 'NN': 2, 'VBD': 1, 'DT': 1, 'CC': 1,...\n",
      "18    {'DT': 1, 'JJR': 1, 'NN': 4, 'IN': 4, 'NNS': 5...\n",
      "19    {'NN': 2, 'VBZ': 1, 'VBN': 1, 'PRP$': 1, 'NNS'...\n",
      "Name: pos_counts, dtype: object\n",
      "Filtered POS Tags: ['JJ', 'NN', 'VBD', 'DT', 'WP', 'IN', 'NNS', 'PRP$', 'WRB', 'TO', 'VB', 'CC', 'RB', 'PRP', 'JJR', 'VBP', 'VBZ', 'VBN', 'EX', 'CD', 'MD', 'VBG', 'RP', 'WDT', 'RBR', 'JJS', 'PDT', 'FW', 'RBS', 'NNPS', 'NNP']\n",
      "\n",
      "Classification Report (Filtered POS Tags):\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "Conflicting Evidence/Cherrypicking       0.45      0.81      0.58       195\n",
      "               Not Enough Evidence       0.70      0.32      0.44       282\n",
      "\n",
      "                          accuracy                           0.52       477\n",
      "                         macro avg       0.57      0.56      0.51       477\n",
      "                      weighted avg       0.60      0.52      0.49       477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'my_env/data/train.json'  # Change to your path\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Filter dataset to include only two labels\n",
    "data = data[data['label'].isin(['Conflicting Evidence/Cherrypicking', 'Not Enough Evidence'])]\n",
    "\n",
    "# Preprocess the text to make it easier to extract words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "\n",
    "# Function to extract POS tags\n",
    "def extract_pos_tags(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "data['pos_tags'] = data['tokens'].apply(extract_pos_tags)\n",
    "\n",
    "# Function to count POS tags in each claim\n",
    "def count_pos_tags(pos_tags):\n",
    "    pos_counts = Counter(pos for word, pos in pos_tags)\n",
    "    return pos_counts\n",
    "\n",
    "# Apply function to count POS tags\n",
    "data['pos_counts'] = data['pos_tags'].apply(count_pos_tags)\n",
    "\n",
    "# Debugging: Verify that 'pos_counts' column exists and is correctly populated\n",
    "print(\"\\nSample of 'pos_counts':\")\n",
    "print(data['pos_counts'].head())\n",
    "\n",
    "# Prepare the dataset for classifier\n",
    "pos_tag_df = pd.DataFrame([{pos: count for pos, count in counter.items()} for counter in data['pos_counts']]).fillna(0)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['label'])\n",
    "\n",
    "# Train a RandomForest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(pos_tag_df, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pd.Series(model.feature_importances_, index=pos_tag_df.columns)\n",
    "filtered_pos_tags = feature_importances[feature_importances > 0.001].index.tolist()  # Keep only features with importance > 0.01\n",
    "print(f\"Filtered POS Tags: {filtered_pos_tags}\")\n",
    "\n",
    "# Update pos_counts to include only filtered POS tags\n",
    "def filter_pos_counts(pos_counts, filtered_pos_tags):\n",
    "    return {pos: count for pos, count in pos_counts.items() if pos in filtered_pos_tags}\n",
    "\n",
    "data['filtered_pos_counts'] = data['pos_counts'].apply(lambda x: filter_pos_counts(x, filtered_pos_tags))\n",
    "\n",
    "# Function to classify a single claim using filtered POS tags\n",
    "coefficients_filtered = {\n",
    "    'Conflicting Evidence/Cherrypicking': {'JJR': 0.415546, 'RP': 0.246184, 'NNS': 0.217197, 'PRP$': 0.206919, 'CC': 0.195953, 'VBN': -0.360008, 'WRB': -0.390771, 'EX': -0.557441, 'PDT': -0.748603, 'JJS': -1.084974},\n",
    "    'Not Enough Evidence': {'NNPS': 1.254270, 'PDT': 0.679914, 'WRB': 0.461062, 'RBR': 0.401490, 'CC': 0.392261, 'JJ': -0.118838, 'PRP$': -0.199226, 'FW': -0.266749, 'PRP': -0.358378, 'NNP': -0.715390}\n",
    "}\n",
    "\n",
    "def classify_claim_filtered(pos_counts):\n",
    "    scores = {label: 0.0 for label in coefficients_filtered.keys()}\n",
    "    for label in coefficients_filtered.keys():\n",
    "        for pos_tag, coef in coefficients_filtered[label].items():\n",
    "            scores[label] += coef * pos_counts.get(pos_tag, 0)\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "# Apply classification to each claim\n",
    "data['predicted_label_filtered'] = data['filtered_pos_counts'].apply(classify_claim_filtered)\n",
    "\n",
    "# Evaluate the classification\n",
    "print(\"\\nClassification Report (Filtered POS Tags):\")\n",
    "print(classification_report(data['label'], data['predicted_label_filtered']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report with Advanced Features:\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "Conflicting Evidence/Cherrypicking       0.06      0.02      0.02        65\n",
      "               Not Enough Evidence       0.14      0.10      0.11        63\n",
      "                           Refuted       0.66      0.75      0.71       555\n",
      "                         Supported       0.41      0.40      0.41       238\n",
      "\n",
      "                          accuracy                           0.57       921\n",
      "                         macro avg       0.32      0.32      0.31       921\n",
      "                      weighted avg       0.52      0.57      0.54       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'my_env/data/train.json'  # Change to your path\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Preprocess the text to make it easier to extract words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "\n",
    "# Function to extract POS tags\n",
    "def extract_pos_tags(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "data['pos_tags'] = data['tokens'].apply(extract_pos_tags)\n",
    "\n",
    "# Function to count POS tags in each claim\n",
    "def count_pos_tags(pos_tags):\n",
    "    pos_counts = Counter(pos for word, pos in pos_tags)\n",
    "    return pos_counts\n",
    "\n",
    "data['pos_counts'] = data['pos_tags'].apply(count_pos_tags)\n",
    "\n",
    "# Function to extract POS n-grams\n",
    "def extract_pos_ngrams(pos_tags, n):\n",
    "    pos_sequence = [pos for word, pos in pos_tags]\n",
    "    ngrams_list = list(ngrams(pos_sequence, n))\n",
    "    return Counter(ngrams_list)\n",
    "\n",
    "data['pos_bigrams'] = data['pos_tags'].apply(lambda x: extract_pos_ngrams(x, 2))\n",
    "data['pos_trigrams'] = data['pos_tags'].apply(lambda x: extract_pos_ngrams(x, 3))\n",
    "\n",
    "# Function to calculate POS ratios\n",
    "def calculate_pos_ratios(pos_counts):\n",
    "    total = sum(pos_counts.values())\n",
    "    ratios = {pos: count / total for pos, count in pos_counts.items()}\n",
    "    return ratios\n",
    "\n",
    "data['pos_ratios'] = data['pos_counts'].apply(calculate_pos_ratios)\n",
    "\n",
    "# Create a combined feature DataFrame\n",
    "pos_counts_df = pd.DataFrame(data['pos_counts'].tolist()).fillna(0)\n",
    "pos_bigrams_df = pd.DataFrame(data['pos_bigrams'].tolist()).fillna(0)\n",
    "pos_trigrams_df = pd.DataFrame(data['pos_trigrams'].tolist()).fillna(0)\n",
    "pos_ratios_df = pd.DataFrame(data['pos_ratios'].tolist()).fillna(0)\n",
    "\n",
    "# Convert tuple column names to strings\n",
    "pos_bigrams_df.columns = pos_bigrams_df.columns.map(str)\n",
    "pos_trigrams_df.columns = pos_trigrams_df.columns.map(str)\n",
    "\n",
    "features_df = pd.concat([pos_counts_df, pos_bigrams_df, pos_trigrams_df, pos_ratios_df], axis=1)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['label'])\n",
    "target_names = label_encoder.classes_\n",
    "\n",
    "# Train a logistic regression model\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, y, test_size=0.3, random_state=42)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nClassification Report with Advanced Features:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./my_env/lib/python3.11/site-packages (from imbalanced-learn) (2.0.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./my_env/lib/python3.11/site-packages (from imbalanced-learn) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in ./my_env/lib/python3.11/site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./my_env/lib/python3.11/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./my_env/lib/python3.11/site-packages (from imbalanced-learn) (3.5.0)\n",
      "Downloading imbalanced_learn-0.12.3-py3-none-any.whl (258 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.3\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report with Advanced Features and SMOTE:\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "Conflicting Evidence/Cherrypicking       0.08      0.09      0.08        65\n",
      "               Not Enough Evidence       0.11      0.21      0.15        63\n",
      "                           Refuted       0.69      0.56      0.62       555\n",
      "                         Supported       0.38      0.44      0.41       238\n",
      "\n",
      "                          accuracy                           0.47       921\n",
      "                         macro avg       0.31      0.33      0.31       921\n",
      "                      weighted avg       0.53      0.47      0.49       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'my_env/data/train.json'  # Change to your path\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Preprocess the text to make it easier to extract words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "\n",
    "# Function to extract POS tags\n",
    "def extract_pos_tags(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "data['pos_tags'] = data['tokens'].apply(extract_pos_tags)\n",
    "\n",
    "# Function to count POS tags in each claim\n",
    "def count_pos_tags(pos_tags):\n",
    "    pos_counts = Counter(pos for word, pos in pos_tags)\n",
    "    return pos_counts\n",
    "\n",
    "data['pos_counts'] = data['pos_tags'].apply(count_pos_tags)\n",
    "\n",
    "# Function to extract POS n-grams\n",
    "def extract_pos_ngrams(pos_tags, n):\n",
    "    pos_sequence = [pos for word, pos in pos_tags]\n",
    "    ngrams_list = list(ngrams(pos_sequence, n))\n",
    "    return Counter(ngrams_list)\n",
    "\n",
    "data['pos_bigrams'] = data['pos_tags'].apply(lambda x: extract_pos_ngrams(x, 2))\n",
    "data['pos_trigrams'] = data['pos_tags'].apply(lambda x: extract_pos_ngrams(x, 3))\n",
    "\n",
    "# Function to calculate POS ratios\n",
    "def calculate_pos_ratios(pos_counts):\n",
    "    total = sum(pos_counts.values())\n",
    "    ratios = {pos: count / total for pos, count in pos_counts.items()}\n",
    "    return ratios\n",
    "\n",
    "data['pos_ratios'] = data['pos_counts'].apply(calculate_pos_ratios)\n",
    "\n",
    "# Create a combined feature DataFrame\n",
    "pos_counts_df = pd.DataFrame(data['pos_counts'].tolist()).fillna(0)\n",
    "pos_bigrams_df = pd.DataFrame(data['pos_bigrams'].tolist()).fillna(0)\n",
    "pos_trigrams_df = pd.DataFrame(data['pos_trigrams'].tolist()).fillna(0)\n",
    "pos_ratios_df = pd.DataFrame(data['pos_ratios'].tolist()).fillna(0)\n",
    "\n",
    "# Convert tuple column names to strings\n",
    "pos_bigrams_df.columns = pos_bigrams_df.columns.map(str)\n",
    "pos_trigrams_df.columns = pos_trigrams_df.columns.map(str)\n",
    "\n",
    "features_df = pd.concat([pos_counts_df, pos_bigrams_df, pos_trigrams_df, pos_ratios_df], axis=1)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['label'])\n",
    "target_names = label_encoder.classes_\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a logistic regression model with class weights\n",
    "class_weights = dict(zip(range(len(target_names)), [1.0] * len(target_names)))\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nClassification Report with Advanced Features and SMOTE:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in ./my_env/lib/python3.11/site-packages (1.14.0)\n",
      "Requirement already satisfied: gensim in ./my_env/lib/python3.11/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in ./my_env/lib/python3.11/site-packages (from scipy) (2.0.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./my_env/lib/python3.11/site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in ./my_env/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (/home/aaronbry/my_env/lib/python3.11/site-packages/scipy/linalg/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[166], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "File \u001b[0;32m~/my_env/lib/python3.11/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m~/my_env/lib/python3.11/site-packages/gensim/corpora/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[0;32m~/my_env/lib/python3.11/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[1;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[0;32m~/my_env/lib/python3.11/site-packages/gensim/interfaces.py:19\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[0;32m~/my_env/lib/python3.11/site-packages/gensim/matutils.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (/home/aaronbry/my_env/lib/python3.11/site-packages/scipy/linalg/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'my_env/data/train.json'  # Change to your path\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "\n",
    "# Train a Word2Vec model on the tokenized claims\n",
    "sentences = data['tokens'].tolist()\n",
    "word2vec_model = gensim.models.Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, seed=42)\n",
    "\n",
    "# Function to convert a list of tokens to a Word2Vec vector\n",
    "def tokens_to_word2vec(tokens, model, vector_size):\n",
    "    vec = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vec += model.wv[token]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "# Apply the function to convert each claim to a Word2Vec vector\n",
    "vector_size = 100\n",
    "data['word2vec'] = data['tokens'].apply(lambda x: tokens_to_word2vec(x, word2vec_model, vector_size))\n",
    "\n",
    "# Create a feature DataFrame from the Word2Vec vectors\n",
    "features_df = pd.DataFrame(data['word2vec'].tolist())\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['label'])\n",
    "target_names = label_encoder.classes_\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a logistic regression model with class weights\n",
    "class_weights = dict(zip(range(len(target_names)), [1.0] * len(target_names)))\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nClassification Report with Word2Vec and SMOTE:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best combination of POS *ignore & all below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/home/aaronbry/my_env/data/train.json'  # Change to your path\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract POS tags\n",
    "def extract_pos_tags(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "data['pos_tags'] = data['tokens'].apply(extract_pos_tags)\n",
    "\n",
    "# Define combinations of POS tags\n",
    "pos_combinations = [\n",
    "    ['JJ', 'JJR', 'JJS'],  # Adjectives\n",
    "    ['RB', 'RBR', 'RBS'],  # Adverbs\n",
    "    ['MD'],  # Modals\n",
    "    ['UH'],  # Interjections\n",
    "    ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'MD', 'UH']  # All combined\n",
    "]\n",
    "\n",
    "# Function to count specific POS tags in tokens\n",
    "def count_pos_tags(pos_tags, pos_list):\n",
    "    return sum(1 for word, pos in pos_tags if pos in pos_list)\n",
    "\n",
    "# Create feature sets for each combination\n",
    "for pos_comb in pos_combinations:\n",
    "    feature_name = '_'.join(pos_comb) + '_count'\n",
    "    data[feature_name] = data['pos_tags'].apply(lambda pos_tags: count_pos_tags(pos_tags, pos_comb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: JJ_JJR_JJS_count, Accuracy: 0.8566775244299675\n",
      "Features: RB_RBR_RBS_count, Accuracy: 0.8566775244299675\n",
      "Features: MD_count, Accuracy: 0.8566775244299675\n",
      "Features: UH_count, Accuracy: 0.8566775244299675\n",
      "Features: JJ_JJR_JJS_RB_RBR_RBS_MD_UH_count, Accuracy: 0.8566775244299675\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset for training\n",
    "X = data[[('_'.join(pos_comb) + '_count') for pos_comb in pos_combinations]]\n",
    "y = data['label'].apply(lambda x: 1 if x in ['Conflicting Evidence/Cherrypicking', 'Not Enough Evidence'] else 0)  # Example labeling\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for pos_comb in pos_combinations:\n",
    "    feature_name = '_'.join(pos_comb) + '_count'\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train[[feature_name]], y_train)\n",
    "    y_pred = model.predict(X_test[[feature_name]])\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[feature_name] = accuracy\n",
    "    print(f\"Features: {feature_name}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best POS combination: JJ_JJR_JJS_count, Accuracy: 0.8566775244299675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92       526\n",
      "           1       0.00      0.00      0.00        88\n",
      "\n",
      "    accuracy                           0.86       614\n",
      "   macro avg       0.43      0.50      0.46       614\n",
      "weighted avg       0.73      0.86      0.79       614\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronbry/my_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/aaronbry/my_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/aaronbry/my_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "best_combination = max(results, key=results.get)\n",
    "print(f\"Best POS combination: {best_combination}, Accuracy: {results[best_combination]}\")\n",
    "\n",
    "# Detailed classification report for the best combination\n",
    "best_model = LogisticRegression()\n",
    "best_model.fit(X_train[[best_combination]], y_train)\n",
    "y_best_pred = best_model.predict(X_test[[best_combination]])\n",
    "print(classification_report(y_test, y_best_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "Refuted                               305\n",
      "Conflicting Evidence/Cherrypicking    216\n",
      "Not Enough Evidence                   211\n",
      "Supported                             122\n",
      "Name: count, dtype: int64\n",
      "Features: JJ_JJR_JJS_count, Accuracy: 0.4678362573099415\n",
      "Features: RB_RBR_RBS_count, Accuracy: 0.5380116959064327\n",
      "Features: MD_count, Accuracy: 0.49707602339181284\n",
      "Features: UH_count, Accuracy: 0.5146198830409356\n",
      "Features: JJ_JJR_JJS_RB_RBR_RBS_MD_UH_count, Accuracy: 0.4853801169590643\n",
      "       JJ_JJR_JJS_count  RB_RBR_RBS_count    MD_count  UH_count  \\\n",
      "count        854.000000        854.000000  854.000000     854.0   \n",
      "mean           1.820843          0.615925    0.160422       0.0   \n",
      "std            1.673036          1.241158    0.415161       0.0   \n",
      "min            0.000000          0.000000    0.000000       0.0   \n",
      "25%            1.000000          0.000000    0.000000       0.0   \n",
      "50%            2.000000          0.000000    0.000000       0.0   \n",
      "75%            3.000000          1.000000    0.000000       0.0   \n",
      "max            8.000000          9.000000    3.000000       0.0   \n",
      "\n",
      "       JJ_JJR_JJS_RB_RBR_RBS_MD_UH_count  \n",
      "count                          854.00000  \n",
      "mean                             2.59719  \n",
      "std                              2.45700  \n",
      "min                              0.00000  \n",
      "25%                              1.00000  \n",
      "50%                              2.00000  \n",
      "75%                              3.00000  \n",
      "max                             15.00000  \n",
      "Combined Features Accuracy: 0.543859649122807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.69      0.61        88\n",
      "           1       0.54      0.39      0.45        83\n",
      "\n",
      "    accuracy                           0.54       171\n",
      "   macro avg       0.54      0.54      0.53       171\n",
      "weighted avg       0.54      0.54      0.53       171\n",
      "\n",
      "Scaled Features Accuracy: 0.543859649122807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.69      0.61        88\n",
      "           1       0.54      0.39      0.45        83\n",
      "\n",
      "    accuracy                           0.54       171\n",
      "   macro avg       0.54      0.54      0.53       171\n",
      "weighted avg       0.54      0.54      0.53       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/home/aaronbry/my_env/data/dev.json'  # Change to your path\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = data[data['label'].isin(['Refuted', 'Supported'])]\n",
    "df_minority = data[data['label'].isin(['Conflicting Evidence/Cherrypicking', 'Not Enough Evidence'])]\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,    # sample with replacement\n",
    "                                 n_samples=len(df_majority),  # to match majority class\n",
    "                                 random_state=42)  # reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "data_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Display new class counts\n",
    "print(data_balanced['label'].value_counts())\n",
    "\n",
    "# Function to extract POS tags\n",
    "def extract_pos_tags(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "data_balanced['pos_tags'] = data_balanced['tokens'].apply(extract_pos_tags)\n",
    "\n",
    "# Define combinations of POS tags\n",
    "pos_combinations = [\n",
    "    ['JJ', 'JJR', 'JJS'],  # Adjectives\n",
    "    ['RB', 'RBR', 'RBS'],  # Adverbs\n",
    "    ['MD'],  # Modals\n",
    "    ['UH'],  # Interjections\n",
    "    ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'MD', 'UH']  # All combined\n",
    "]\n",
    "\n",
    "# Function to count specific POS tags in tokens\n",
    "def count_pos_tags(pos_tags, pos_list):\n",
    "    return sum(1 for word, pos in pos_tags if pos in pos_list)\n",
    "\n",
    "# Create feature sets for each combination\n",
    "for pos_comb in pos_combinations:\n",
    "    feature_name = '_'.join(pos_comb) + '_count'\n",
    "    data_balanced[feature_name] = data_balanced['pos_tags'].apply(lambda pos_tags: count_pos_tags(pos_tags, pos_comb))\n",
    "\n",
    "# Prepare the dataset for training\n",
    "X = data_balanced[[('_'.join(pos_comb) + '_count') for pos_comb in pos_combinations]]\n",
    "y = data_balanced['label'].apply(lambda x: 1 if x in ['Conflicting Evidence/Cherrypicking', 'Not Enough Evidence'] else 0)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate models with class weights\n",
    "results = {}\n",
    "for pos_comb in pos_combinations:\n",
    "    feature_name = '_'.join(pos_comb) + '_count'\n",
    "    model = LogisticRegression(class_weight='balanced')\n",
    "    model.fit(X_train[[feature_name]], y_train)\n",
    "    y_pred = model.predict(X_test[[feature_name]])\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[feature_name] = accuracy\n",
    "    print(f\"Features: {feature_name}, Accuracy: {accuracy}\")\n",
    "\n",
    "# Check feature distribution\n",
    "print(data_balanced[['JJ_JJR_JJS_count', 'RB_RBR_RBS_count', 'MD_count', 'UH_count', 'JJ_JJR_JJS_RB_RBR_RBS_MD_UH_count']].describe())\n",
    "\n",
    "# Combine all features\n",
    "X_combined = data_balanced[['JJ_JJR_JJS_count', 'RB_RBR_RBS_count', 'MD_count', 'UH_count', 'JJ_JJR_JJS_RB_RBR_RBS_MD_UH_count']]\n",
    "X_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model on combined features with class weights\n",
    "combined_model = LogisticRegression(class_weight='balanced')\n",
    "combined_model.fit(X_train_combined, y_train_combined)\n",
    "y_combined_pred = combined_model.predict(X_test_combined)\n",
    "combined_accuracy = accuracy_score(y_test_combined, y_combined_pred)\n",
    "print(f\"Combined Features Accuracy: {combined_accuracy}\")\n",
    "\n",
    "# Detailed classification report for the combined features\n",
    "print(classification_report(y_test_combined, y_combined_pred))\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_combined)\n",
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model on scaled features with class weights\n",
    "scaled_model = LogisticRegression(class_weight='balanced')\n",
    "scaled_model.fit(X_train_scaled, y_train_scaled)\n",
    "y_scaled_pred = scaled_model.predict(X_test_scaled)\n",
    "scaled_accuracy = accuracy_score(y_test_scaled, y_scaled_pred)\n",
    "print(f\"Scaled Features Accuracy: {scaled_accuracy}\")\n",
    "\n",
    "# Detailed classification report for the scaled features\n",
    "print(classification_report(y_test_scaled, y_scaled_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Relevant POS Tags for 'Conflicting Evidence/Cherrypicking':\n",
      "   POS Tag  Frequency\n",
      "0      JJ       2690\n",
      "1      RB        701\n",
      "2      MD        300\n",
      "3     JJR         87\n",
      "4     JJS         65\n",
      "5     RBR         26\n",
      "6     RBS         17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/home/aaronbry/my_env/data/train.json'  # Change to your path\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Preprocess the text to make it easier to extract words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "\n",
    "# Function to extract POS tags\n",
    "def extract_pos_tags(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "data['pos_tags'] = data['tokens'].apply(extract_pos_tags)\n",
    "\n",
    "# Filter claims by specific labels\n",
    "conflicting_claims = data[data['label'] == 'Refuted']\n",
    "\n",
    "# Relevant POS tags typically associated with opinionated statements\n",
    "relevant_pos_tags = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'MD', 'UH']\n",
    "\n",
    "# Function to filter and count relevant POS tags\n",
    "def filter_relevant_pos_tags(pos_tags, relevant_pos_tags):\n",
    "    return [pos for word, pos in pos_tags if pos in relevant_pos_tags]\n",
    "\n",
    "# Apply filtering to get relevant POS tags for conflicting claims\n",
    "conflicting_relevant_pos_tags = [pos for tokens in conflicting_claims['pos_tags'] for pos in filter_relevant_pos_tags(tokens, relevant_pos_tags)]\n",
    "\n",
    "# Frequency distribution of relevant POS tags\n",
    "conflicting_relevant_pos_freq = Counter(conflicting_relevant_pos_tags)\n",
    "\n",
    "# Get the most common relevant POS tags\n",
    "most_common_conflicting_relevant_pos = conflicting_relevant_pos_freq.most_common(20)\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "most_common_conflicting_relevant_pos_df = pd.DataFrame(most_common_conflicting_relevant_pos, columns=['POS Tag', 'Frequency'])\n",
    "\n",
    "# Display the results\n",
    "print(\"Most Common Relevant POS Tags for 'Conflicting Evidence/Cherrypicking':\\n\", most_common_conflicting_relevant_pos_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
