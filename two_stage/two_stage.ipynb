{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both Models classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels written to my_env/two_stage/results/predicted_claims_both_modelsrandomforest.json\n",
      "Accuracy for 'Refuted': 0.31\n",
      "Accuracy for 'Supported': 0.20\n",
      "Accuracy for 'Not Enough Evidence': 0.46\n",
      "Accuracy for 'Conflicting Evidence/Cherrypicking': 0.34\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_models_and_vectorizers(selection):\n",
    "    \"\"\"\n",
    "    Load the main and secondary models and preprocessing objects.\n",
    "    \"\"\"\n",
    "    with open(f'my_env/two_stage/models_1/all_data_model_{selection}.pkl', 'rb') as model_file:\n",
    "        main_model = pickle.load(model_file)\n",
    "\n",
    "    with open(f'my_env/two_stage/models_1/all_data_tfidf_vectorizer_{selection}.pkl', 'rb') as vectorizer_file:\n",
    "        main_vectorizer = pickle.load(vectorizer_file)\n",
    "\n",
    "    with open(f'my_env/two_stage/models_1/all_data_label_map_{selection}.pkl', 'rb') as label_map_file:\n",
    "        main_label_map = pickle.load(label_map_file)\n",
    "\n",
    "    with open(f'my_env/two_stage/models_1/nee_cp_model_{selection}.pkl', 'rb') as secondary_model_file:\n",
    "        secondary_model = pickle.load(secondary_model_file)\n",
    "\n",
    "    with open(f'my_env/two_stage/models_1/nee_cptfidf_vectorizer_{selection}.pkl', 'rb') as secondary_vectorizer_file:\n",
    "        secondary_vectorizer = pickle.load(secondary_vectorizer_file)\n",
    "\n",
    "    with open(f'my_env/two_stage/models_1/nee_cplabel_map_{selection}.pkl', 'rb') as secondary_label_map_file:\n",
    "        secondary_label_map = pickle.load(secondary_label_map_file)\n",
    "\n",
    "    return main_model, main_vectorizer, main_label_map, secondary_model, secondary_vectorizer, secondary_label_map\n",
    "\n",
    "def preprocess_claim(claim, vectorizer):\n",
    "    \"\"\"\n",
    "    Preprocess claims using the respective vectorizers.\n",
    "    \"\"\"\n",
    "    return vectorizer.transform([claim]).toarray()\n",
    "\n",
    "def classify_main(claim, vectorizer, model):\n",
    "    \"\"\"\n",
    "    Classify claims using the main model.\n",
    "    \"\"\"\n",
    "    features = preprocess_claim(claim, vectorizer)\n",
    "    prediction = model.predict(features)\n",
    "    probability_distribution = model.predict_proba(features)\n",
    "    confidence = np.max(probability_distribution)\n",
    "    return prediction[0], confidence, probability_distribution[0]\n",
    "\n",
    "def classify_secondary(claim, vectorizer, model):\n",
    "    \"\"\"\n",
    "    Classify claims using the secondary model.\n",
    "    \"\"\"\n",
    "    features = preprocess_claim(claim, vectorizer)\n",
    "    prediction = model.predict(features)\n",
    "    probability_distribution = model.predict_proba(features)\n",
    "    return prediction[0], probability_distribution[0]\n",
    "\n",
    "def predict_single_claim(claim, main_model, main_vectorizer, main_label_map_inverse, secondary_model, secondary_vectorizer, secondary_label_map_inverse):\n",
    "    \"\"\"\n",
    "    Two-step classification process for a single claim.\n",
    "    \"\"\"\n",
    "    predicted_label, confidence, probability_distribution = classify_main(claim, main_vectorizer, main_model)\n",
    "    \n",
    "    if main_label_map_inverse[predicted_label] in ['Supported', 'Refuted'] and confidence >= 0.40:\n",
    "        # High confidence in main model's prediction\n",
    "        predicted_label_name = main_label_map_inverse[predicted_label]\n",
    "        class_probabilities = {main_label_map_inverse[i]: prob for i, prob in enumerate(probability_distribution)}\n",
    "    else:\n",
    "        # Use secondary model for further classification\n",
    "        predicted_label, probability_distribution = classify_secondary(claim, secondary_vectorizer, secondary_model)\n",
    "        predicted_label_name = secondary_label_map_inverse[predicted_label]\n",
    "        class_probabilities = {secondary_label_map_inverse[i]: prob for i, prob in enumerate(probability_distribution)}\n",
    "\n",
    "    return predicted_label_name, class_probabilities, confidence\n",
    "\n",
    "def predict_dataset(input_file_path, output_file_path, main_model, main_vectorizer, main_label_map_inverse, secondary_model, secondary_vectorizer, secondary_label_map_inverse):\n",
    "    \"\"\"\n",
    "    Function to iterate through the dataset and predict labels.\n",
    "    \"\"\"\n",
    "    df = pd.read_json(input_file_path)\n",
    "    results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        claim = row['claim']\n",
    "        actual_label = row['label']\n",
    "        predicted_label, class_probabilities, confidence = predict_single_claim(claim, main_model, main_vectorizer, main_label_map_inverse, secondary_model, secondary_vectorizer, secondary_label_map_inverse)\n",
    "        result = {\n",
    "            'claim': claim,\n",
    "            'actual_label': actual_label,\n",
    "            'predicted_label': predicted_label,\n",
    "            'confidence': confidence,\n",
    "            'class_probabilities': class_probabilities\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        json.dump(results, outfile, indent=4)\n",
    "\n",
    "def calculate_accuracy_from_json(input_file):\n",
    "    \"\"\"\n",
    "    Function to calculate accuracy per label.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r') as file:\n",
    "        results = json.load(file)\n",
    "    \n",
    "    correct_predictions = {}\n",
    "    total_predictions = {}\n",
    "    \n",
    "    for result in results:\n",
    "        actual_label = result['actual_label']\n",
    "        predicted_label = result['predicted_label']\n",
    "        \n",
    "        if actual_label not in correct_predictions:\n",
    "            correct_predictions[actual_label] = 0\n",
    "            total_predictions[actual_label] = 0\n",
    "            \n",
    "        if predicted_label == actual_label:\n",
    "            correct_predictions[actual_label] += 1\n",
    "        \n",
    "        total_predictions[actual_label] += 1\n",
    "    \n",
    "    accuracy_per_label = {label: correct_predictions[label] / total_predictions[label] for label in total_predictions}\n",
    "    return accuracy_per_label\n",
    "\n",
    "def main():\n",
    "    selection = 'randomforest' # or 'logreg' | defines which model you use\n",
    "    json_output_filename = f\"predicted_claims_both_models{selection}\"\n",
    "    input_file_path = 'my_env/two_stage/data/dev.json' # or 'my_env/two_stage/data/data_test.json' for test set\n",
    "    output_file_path = f'my_env/two_stage/results/{json_output_filename}.json'\n",
    "\n",
    "    main_model, main_vectorizer, main_label_map, secondary_model, secondary_vectorizer, secondary_label_map = load_models_and_vectorizers(selection)\n",
    "    main_label_map_inverse = {v: k for k, v in main_label_map.items()}\n",
    "    secondary_label_map_inverse = {v: k for k, v in secondary_label_map.items()}\n",
    "    predict_dataset(input_file_path, output_file_path, main_model, main_vectorizer, main_label_map_inverse, secondary_model, secondary_vectorizer, secondary_label_map_inverse)\n",
    "    print(f\"Predicted labels written to {output_file_path}\")\n",
    "    accuracy_per_label = calculate_accuracy_from_json(output_file_path)\n",
    "    for label, accuracy in accuracy_per_label.items():\n",
    "        print(f\"Accuracy for '{label}': {accuracy:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both Models on a single claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Conflicting Evidence/Cherrypicking | Label Confidence: 0.40283489805794537\n",
      "Class Probabilities: {'Conflicting Evidence/Cherrypicking': 0.6008023586993227, 'Not Enough Evidence': 0.3991976413006784}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def load_models_and_vectorizers(selection):\n",
    "    \"\"\"\n",
    "    Load the main and secondary models and preprocessing objects.\n",
    "    \"\"\"\n",
    "    with open(f'my_env/two_stage/models_1/all_data_model_{selection}.pkl', 'rb') as model_file:\n",
    "        main_model = pickle.load(model_file)\n",
    "\n",
    "    with open(f'my_env/two_stage/models_1/all_data_tfidf_vectorizer_{selection}.pkl', 'rb') as vectorizer_file:\n",
    "        main_vectorizer = pickle.load(vectorizer_file)\n",
    "\n",
    "    with open(f'my_env/two_stage/models_1/all_data_label_map_{selection}.pkl', 'rb') as label_map_file:\n",
    "        main_label_map = pickle.load(label_map_file)\n",
    "\n",
    "    with open(f'my_env/two_stage/models_1/nee_cp_model_{selection}.pkl', 'rb') as secondary_model_file:\n",
    "        secondary_model = pickle.load(secondary_model_file)\n",
    "\n",
    "    with open(f'my_env/two_stage/models_1/nee_cptfidf_vectorizer_{selection}.pkl', 'rb') as secondary_vectorizer_file:\n",
    "        secondary_vectorizer = pickle.load(secondary_vectorizer_file)\n",
    "\n",
    "    with open(f'my_env/two_stage/models_1/nee_cplabel_map_{selection}.pkl', 'rb') as secondary_label_map_file:\n",
    "        secondary_label_map = pickle.load(secondary_label_map_file)\n",
    "\n",
    "    return main_model, main_vectorizer, main_label_map, secondary_model, secondary_vectorizer, secondary_label_map\n",
    "\n",
    "def preprocess_claim(claim, vectorizer):\n",
    "    \"\"\"\n",
    "    Preprocess claims using the respective vectorizers.\n",
    "    \"\"\"\n",
    "    return vectorizer.transform([claim]).toarray()\n",
    "\n",
    "def classify_main(claim, vectorizer, model):\n",
    "    \"\"\"\n",
    "    Classify claims using the main model.\n",
    "    \"\"\"\n",
    "    features = preprocess_claim(claim, vectorizer)\n",
    "    prediction = model.predict(features)\n",
    "    probability_distribution = model.predict_proba(features)\n",
    "    confidence = np.max(probability_distribution)\n",
    "    return prediction[0], confidence, probability_distribution[0]\n",
    "\n",
    "def classify_secondary(claim, vectorizer, model):\n",
    "    \"\"\"\n",
    "    Classify claims using the secondary model.\n",
    "    \"\"\"\n",
    "    features = preprocess_claim(claim, vectorizer)\n",
    "    prediction = model.predict(features)\n",
    "    probability_distribution = model.predict_proba(features)\n",
    "    return prediction[0], probability_distribution[0]\n",
    "\n",
    "def predict_single_claim(claim, main_model, main_vectorizer, main_label_map_inverse, secondary_model, secondary_vectorizer, secondary_label_map_inverse):\n",
    "    \"\"\"\n",
    "    Two-step classification process for a single claim.\n",
    "    \"\"\"\n",
    "    predicted_label, confidence, probability_distribution = classify_main(claim, main_vectorizer, main_model)\n",
    "    \n",
    "    if main_label_map_inverse[predicted_label] in ['Supported', 'Refuted'] and confidence >= 0.60:\n",
    "        # High confidence in main model's prediction\n",
    "        predicted_label_name = main_label_map_inverse[predicted_label]\n",
    "        class_probabilities = {main_label_map_inverse[i]: prob for i, prob in enumerate(probability_distribution)}\n",
    "    else:\n",
    "        # Use secondary model for further classification\n",
    "        predicted_label, probability_distribution = classify_secondary(claim, secondary_vectorizer, secondary_model)\n",
    "        predicted_label_name = secondary_label_map_inverse[predicted_label]\n",
    "        class_probabilities = {secondary_label_map_inverse[i]: prob for i, prob in enumerate(probability_distribution)}\n",
    "\n",
    "    return predicted_label_name, class_probabilities, confidence\n",
    "\n",
    "def main():\n",
    "\n",
    "    single_claim = \"Donald Trump said that $15 an hour is too much for essential workers\"\n",
    "\n",
    "    selection = 'randomforest'  # or 'logreg' | defines which model you use\n",
    "\n",
    "    main_model, main_vectorizer, main_label_map, secondary_model, secondary_vectorizer, secondary_label_map = load_models_and_vectorizers(selection)\n",
    "    main_label_map_inverse = {v: k for k, v in main_label_map.items()}\n",
    "    secondary_label_map_inverse = {v: k for k, v in secondary_label_map.items()}\n",
    "\n",
    "    \n",
    "    predicted_label, class_probabilities, confidence = predict_single_claim(single_claim, main_model, main_vectorizer, main_label_map_inverse, secondary_model, secondary_vectorizer, secondary_label_map_inverse)\n",
    "    \n",
    "    print(f\"Predicted Label: {predicted_label} | Label Confidence: {confidence}\")\n",
    "    print(f\"Class Probabilities: {class_probabilities}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Secondary Model classifying\n",
    "\n",
    "might be useful if claims are narrowed down enough from Support / Refute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels written to my_env/two_stage/results/secondary_model_predictions_on_devrandomforest.json\n",
      "Accuracy for 'Refuted': 0.00\n",
      "Accuracy for 'Supported': 0.00\n",
      "Accuracy for 'Not Enough Evidence': 0.60\n",
      "Accuracy for 'Conflicting Evidence/Cherrypicking': 0.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_model_and_vectorizer(selection):\n",
    "    \"\"\"\n",
    "    Load the secondary model and preprocessing objects.\n",
    "    \"\"\"\n",
    "    with open(f'my_env/two_stage/models_1/nee_cp_model_{selection}.pkl', 'rb') as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "\n",
    "    with open(f'my_env/two_stage/models_1/nee_cptfidf_vectorizer_{selection}.pkl', 'rb') as vectorizer_file:\n",
    "        vectorizer = pickle.load(vectorizer_file)\n",
    "\n",
    "    with open(f'my_env/two_stage/models_1/nee_cplabel_map_{selection}.pkl', 'rb') as label_map_file:\n",
    "        label_map = pickle.load(label_map_file)\n",
    "\n",
    "    return model, vectorizer, label_map\n",
    "\n",
    "def preprocess_claim(claim, vectorizer):\n",
    "    \"\"\"\n",
    "    Preprocess claims using the respective vectorizers.\n",
    "    \"\"\"\n",
    "    return vectorizer.transform([claim]).toarray()\n",
    "\n",
    "def classify_secondary(claim, vectorizer, model):\n",
    "    \"\"\"\n",
    "    Classify claims using the secondary model.\n",
    "    \"\"\"\n",
    "    features = preprocess_claim(claim, vectorizer)\n",
    "    prediction = model.predict(features)\n",
    "    probability_distribution = model.predict_proba(features)\n",
    "    confidence = np.max(probability_distribution)\n",
    "    return prediction[0], confidence, probability_distribution[0]\n",
    "\n",
    "def predict_single_claim(claim, vectorizer, model, label_map_inverse):\n",
    "    \"\"\"\n",
    "    Two-step classification process for a single claim.\n",
    "    \"\"\"\n",
    "    predicted_label, confidence, probability_distribution = classify_secondary(claim, vectorizer, model)\n",
    "    predicted_label_name = label_map_inverse[predicted_label]\n",
    "    class_probabilities = {label_map_inverse[i]: prob for i, prob in enumerate(probability_distribution)}\n",
    "\n",
    "    return predicted_label_name, class_probabilities, confidence\n",
    "\n",
    "def predict_dataset(input_file_path, output_file_path, vectorizer, model, label_map_inverse):\n",
    "    \"\"\"\n",
    "    Function to iterate through the dataset and predict labels.\n",
    "    \"\"\"\n",
    "    df = pd.read_json(input_file_path)\n",
    "    results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        claim = row['claim']\n",
    "        actual_label = row['label']\n",
    "        predicted_label, class_probabilities, confidence = predict_single_claim(claim, vectorizer, model, label_map_inverse)\n",
    "        result = {\n",
    "            'claim': claim,\n",
    "            'actual_label': actual_label,\n",
    "            'predicted_label': predicted_label,\n",
    "            'confidence': confidence,\n",
    "            'class_probabilities': class_probabilities\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        json.dump(results, outfile, indent=4)\n",
    "\n",
    "def calculate_accuracy_from_json(input_file):\n",
    "    \"\"\"\n",
    "    Function to calculate accuracy per label.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r') as file:\n",
    "        results = json.load(file)\n",
    "    \n",
    "    correct_predictions = {}\n",
    "    total_predictions = {}\n",
    "    \n",
    "    for result in results:\n",
    "        actual_label = result['actual_label']\n",
    "        predicted_label = result['predicted_label']\n",
    "        \n",
    "        if actual_label not in correct_predictions:\n",
    "            correct_predictions[actual_label] = 0\n",
    "            total_predictions[actual_label] = 0\n",
    "            \n",
    "        if predicted_label == actual_label:\n",
    "            correct_predictions[actual_label] += 1\n",
    "        \n",
    "        total_predictions[actual_label] += 1\n",
    "    \n",
    "    accuracy_per_label = {label: correct_predictions[label] / total_predictions[label] for label in total_predictions}\n",
    "    return accuracy_per_label\n",
    "\n",
    "def main():\n",
    "    selection = 'randomforest' # or 'logreg' for faster\n",
    "    json_output_filename = f\"secondary_model_predictions_on_dev{selection}\"\n",
    "    input_file_path = 'my_env/two_stage/data/dev.json' # or 'my_env/two_stage/data/data_test.json' for test set\n",
    "    output_file_path = f'my_env/two_stage/results/{json_output_filename}.json'\n",
    "    \n",
    "    model, vectorizer, label_map = load_model_and_vectorizer(selection)\n",
    "    label_map_inverse = {v: k for k, v in label_map.items()}\n",
    "    predict_dataset(input_file_path, output_file_path, vectorizer, model, label_map_inverse)\n",
    "    print(f\"Predicted labels written to {output_file_path}\")\n",
    "    accuracy_per_label = calculate_accuracy_from_json(output_file_path)\n",
    "    for label, accuracy in accuracy_per_label.items():\n",
    "        print(f\"Accuracy for '{label}': {accuracy:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
