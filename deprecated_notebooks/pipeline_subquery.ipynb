{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from averitec import Datapoint\n",
    "from evidence_generation import GptEvidenceGenerator, GptBatchedEvidenceGenerator, DynamicFewShotBatchedEvidenceGenerator\n",
    "from classification import DefaultClassifier, HuggingfaceClassifier, AverageEnsembleClassifier, LogRegEnsembleClassifier\n",
    "from retrieval import SimpleFaissRetriever, Retriever, MmrFaissRetriever, SubqueryRetriever\n",
    "from pipeline import Pipeline, MockPipeline\n",
    "import pickle\n",
    "from labels import label2id, id2label\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "random.seed(111)\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"dev\"\n",
    "path = \"/mnt/data/factcheck/averitec-data/\"\n",
    "with open(path + f\"data/{split}.json\") as f:\n",
    "    dataset = json.load(f)\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i][\"claim_id\"] = i\n",
    "    datapoints = [Datapoint.from_dict(d) for d in dataset]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint = Datapoint.from_dict(dataset[150])\n",
    "datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = SimpleFaissRetriever(path=\"/mnt/data/factcheck/averitec-data/data_store/vecstore/dev/6k\")\n",
    "retriever = MmrFaissRetriever(path=f\"/mnt/data/factcheck/averitec-data/data_store/vecstore/{split}/6k\")\n",
    "#retriever = MmrFaissRetriever(path=f\"/mnt/data/factcheck/averitec-data/data_store/vecstore/{split}/2k\")\n",
    "retrieval_result = retriever(datapoint)\n",
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint = Datapoint.from_dict(dataset[2])\n",
    "datapoint.claim, datapoint.claim_date, datapoint.speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps({\"question\": datapoint.claim, \"context\": datapoint.speaker})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 44975.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from classification import DefaultClassifier, HuggingfaceClassifier, AverageEnsembleClassifier, LogRegEnsembleClassifier, RandomForestClassifier, NoTiebreakClassifier\n",
    "\n",
    "path = \"/mnt/data/factcheck/averitec-data/\"\n",
    "#target = path + \"data_store/vecstore/test/2k\"\n",
    "PIPELINE_NAME = \"mmr+gpt4o-dfewshot-atype\"\n",
    "classifier = DefaultClassifier()\n",
    "if True:\n",
    "    pipeline = MockPipeline(\n",
    "        dumps=f\"/mnt/data/factcheck/averitec-data/data_store/submissions/{split}_mmr+gpt4o-dfewshot-tiebrk-atype.pkl\",\n",
    "        classifier=NoTiebreakClassifier()\n",
    "    )\n",
    "else:\n",
    "    pipeline = Pipeline(\n",
    "        #dumps = \"/mnt/data/factcheck/averitec-data/data_store/submissions/dev_mmr+gpt4o-dfewshot.pkl\",\n",
    "        #SubqueryRetriever(retriever),\n",
    "        retriever,\n",
    "        evidence_generator=DynamicFewShotBatchedEvidenceGenerator(), \n",
    "        classifier=classifier\n",
    "    )\n",
    "\n",
    "submission = []\n",
    "dump = []\n",
    "\n",
    "for dp in tqdm(datapoints):\n",
    "    pipeline_result = pipeline(dp)\n",
    "    submission.append(pipeline_result.to_submission())\n",
    "    dump.append(pipeline_result)\n",
    "with open(f\"/mnt/data/factcheck/averitec-data/data_store/submissions/{split}_{PIPELINE_NAME}.json\", \"w\") as f:\n",
    "    json.dump(submission, f, indent=4)\n",
    "with open(f\"/mnt/data/factcheck/averitec-data/data_store/submissions/{split}_{PIPELINE_NAME}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dump, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_batch_files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevidence_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_files\u001b[49m(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/factcheck/averitec-data/data_store/batch_jobs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPIPELINE_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_batch_files'"
     ]
    }
   ],
   "source": [
    "files = pipeline.evidence_generator.get_batch_files(path=f\"/mnt/data/factcheck/averitec-data/data_store/batch_jobs/{split}_{PIPELINE_NAME}\", batch_size=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_results = pipeline.evidence_generator.submit_and_await_batches(files, f\"/mnt/data/factcheck/averitec-data/data_store/batch_jobs/{split}_{PIPELINE_NAME}/output.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dump = []\n",
    "pipeline.evidence_generator.fallback_gpt_generator.client.temperature = .5\n",
    "for pipeline_result, batch_result in zip(dump[:len(batch_results)], batch_results):\n",
    "    new_result = pipeline.evidence_generator.update_pipeline_result(pipeline_result, batch_result, pipeline.classifier)\n",
    "    new_dump.append(new_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    new_dump[1],\n",
    "    new_dump[1].evidence_generation_result,\n",
    "    new_dump[1].classification_result\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/mnt/data/factcheck/averitec-data/data_store/submissions/{split}_{PIPELINE_NAME}.json\", \"w\") as f:\n",
    "    json.dump([d.to_submission() for d in new_dump], f, indent=4)\n",
    "with open(f\"/mnt/data/factcheck/averitec-data/data_store/submissions/{split}_{PIPELINE_NAME}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(new_dump, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_mmr+gpt4o-dfewshot-atype\n",
      "Question-only score (HU-meteor):             0.4566753100185953\n",
      "Question-answer score (HU-meteor):           0.28764311409757226\n",
      "====================\n",
      "Veracity F1 scores:\n",
      " * Supported:                                0.7252747252747253\n",
      " * Refuted:                                  0.8217821782178217\n",
      " * Not Enough Evidence:                      0.23684210526315788\n",
      " * Conflicting Evidence/Cherrypicking:       0.044444444444444446\n",
      " * macro:                                    0.45708586330003736\n",
      " * acc:                                      0.716\n",
      "--------------------\n",
      "AVeriTeC scores:\n",
      " * Veracity scores (meteor @ 0.1):           0.696\n",
      " * Veracity scores (meteor @ 0.2):           0.546\n",
      " * Veracity scores (meteor @ 0.25):          0.418\n",
      " * Veracity scores (meteor @ 0.3):           0.3\n",
      " * Veracity scores (meteor @ 0.4):           0.136\n",
      " * Veracity scores (meteor @ 0.5):           0.052\n",
      "--------------------\n",
      "AVeriTeC scores by type @ 0.25:\n",
      " * Veracity scores (Event/Property Claim):   0.2076607110651121\n",
      " * Veracity scores (Position Statement):     0.24566208780116447\n",
      " * Veracity scores (Causal Claim):           0.17872845121331243\n",
      " * Veracity scores (Numerical Claim):        0.2312066927954962\n",
      " * Veracity scores (Quote Verification):     0.17159273525866922\n"
     ]
    }
   ],
   "source": [
    "print(f\"{split}_{PIPELINE_NAME}\")\n",
    "%run src/prediction/evaluate_veracity.py --label_file /mnt/data/factcheck/averitec-data/data/dev.json --prediction_file /mnt/data/factcheck/averitec-data/data_store/submissions/{split}_{PIPELINE_NAME}.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_mmr+gpt4o-dfewshot-tiebrk-atype\n",
      "Question-only score (HU-meteor):             0.4566753100185953\n",
      "Question-answer score (HU-meteor):           0.2878181626887146\n",
      "====================\n",
      "Veracity F1 scores:\n",
      " * Supported:                                0.7368421052631579\n",
      " * Refuted:                                  0.813953488372093\n",
      " * Not Enough Evidence:                      0.2222222222222222\n",
      " * Conflicting Evidence/Cherrypicking:       0.0784313725490196\n",
      " * macro:                                    0.46286229710162313\n",
      " * acc:                                      0.708\n",
      "--------------------\n",
      "AVeriTeC scores:\n",
      " * Veracity scores (meteor @ 0.1):           0.688\n",
      " * Veracity scores (meteor @ 0.2):           0.54\n",
      " * Veracity scores (meteor @ 0.25):          0.414\n",
      " * Veracity scores (meteor @ 0.3):           0.292\n",
      " * Veracity scores (meteor @ 0.4):           0.132\n",
      " * Veracity scores (meteor @ 0.5):           0.052\n",
      "--------------------\n",
      "AVeriTeC scores by type @ 0.25:\n",
      " * Veracity scores (Event/Property Claim):   0.2071862837392941\n",
      " * Veracity scores (Position Statement):     0.24902632031305175\n",
      " * Veracity scores (Causal Claim):           0.1744179187790451\n",
      " * Veracity scores (Numerical Claim):        0.23190823143116548\n",
      " * Veracity scores (Quote Verification):     0.17478359308805957\n"
     ]
    }
   ],
   "source": [
    "print(f\"{split}_{PIPELINE_NAME}\")\n",
    "%run src/prediction/evaluate_veracity.py --label_file /mnt/data/factcheck/averitec-data/data/dev.json --prediction_file /mnt/data/factcheck/averitec-data/data_store/submissions/{split}_{PIPELINE_NAME}.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collapsible begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_retrieval_result = retriever(datapoint)\n",
    "display(Markdown(\"### 🗯️ \" + datapoint.claim))\n",
    "display(Markdown(\"*Retrieved by knn*\\n\\n\"))\n",
    "# sample 3\n",
    "for r in knn_retrieval_result:\n",
    "    newline = \"\\n\"\n",
    "    display(Markdown(f\"**{r.metadata['url']}**\\n\\n{r.page_content[:256]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval import MmrFaissRetriever\n",
    "\n",
    "mmr_retriever = MmrFaissRetriever(retriever.path)\n",
    "mmr_retrieval_result = mmr_retriever(datapoint)\n",
    "display(Markdown(\"### 🗯️ \" + datapoint.claim))\n",
    "display(Markdown(\"*Retrieved by MMR*\\n\\n\"))\n",
    "# sample 3\n",
    "for r in mmr_retrieval_result:\n",
    "    newline = \"\\n\"\n",
    "    display(Markdown(f\"**{r.metadata['url']}**\\n\\n{r.page_content[:256]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subquery_retriever = SubqueryRetriever(retriever)\n",
    "subquery_retrieval_result = subquery_retriever(datapoint)\n",
    "display(Markdown(\"### 🗯️ \" + datapoint.claim))\n",
    "display(Markdown(\"*Retrieved by subqueries*\\n\\n\"))\n",
    "# sample 3\n",
    "for r in subquery_retrieval_result:\n",
    "    newline = \"\\n\"\n",
    "    display(Markdown(f\"**{r.metadata['url']}**\\n\\n*{';'.join(r.metadata['queries'])}*\\n\\n{r.page_content[:256]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subquery_retrieval_result.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collapsible section end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_generator = GptBatchedEvidenceGenerator(\"gpt-4o\")\n",
    "evidence_generation_result = evidence_generator(datapoint, retrieval_result)\n",
    "evidence_generation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_generation_result.metadata[\"suggested_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DefaultClassifier()\n",
    "classification_result = classifier(datapoint, evidence_generation_result, retrieval_result)\n",
    "str(classification_result), classification_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint2 = Datapoint.from_dict(dataset[16])\n",
    "pipeline = Pipeline(retriever, evidence_generator, classifier)\n",
    "pipeline_result = pipeline(datapoint2)\n",
    "pipeline_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(pipeline_result.classification_result), datapoint2.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result.to_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle dump pipeline result\n",
    "import pickle\n",
    "with open('data/pipeline_result.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/prediction/evaluate_veracity.py --label_file /mnt/data/factcheck/averitec-data/data/dev.json --prediction_file /mnt/data/factcheck/averitec-data/data_store/submission_dev_avg_clf.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
