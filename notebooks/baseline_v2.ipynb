{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVeriTec Baseline Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⬇️ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "dotenv_path = Path('aic_averitec/.env')\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "DATASTORE_PATH = os.environ.get(\"DATASTORE_PATH\")\n",
    "DATASET_PATH = os.environ.get(\"DATASET_PATH\")\n",
    "MODELS_PATH = os.environ.get(\"MODELS_PATH\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "DEV_PATH = str(os.path.join(DATASET_PATH, 'dev.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rank the sentences in the knowledge store with BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aic_averitec.src.reranking.bm25_sentences import get_top_k_sentences_bm25 #bm25 reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_sentences_bm25(knowledge_store_dir=str(os.path.join(DATASTORE_PATH, 'output_dev')), claim_file=DEV_PATH, json_output=\"./aic_averitec/data_store/dev_top_k.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rank whole texts with neural reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing claim 0... Progress: 1 / 10\n",
      "Obtained 825 documents from 825 urls.\n",
      "Top 10 retrieved. Time elapsed: 29.858542919158936.\n",
      "Processing claim 1... Progress: 2 / 10\n",
      "Obtained 714 documents from 714 urls.\n",
      "Top 10 retrieved. Time elapsed: 23.82103157043457.\n",
      "Processing claim 2... Progress: 3 / 10\n",
      "Obtained 1300 documents from 1300 urls.\n",
      "Top 10 retrieved. Time elapsed: 71.80752301216125.\n",
      "Processing claim 3... Progress: 4 / 10\n",
      "Obtained 1041 documents from 1041 urls.\n",
      "Top 10 retrieved. Time elapsed: 59.356168270111084.\n",
      "Processing claim 4... Progress: 5 / 10\n",
      "Obtained 1064 documents from 1064 urls.\n",
      "Top 10 retrieved. Time elapsed: 33.1887104511261.\n",
      "Processing claim 5... Progress: 6 / 10\n",
      "Obtained 822 documents from 822 urls.\n",
      "Top 10 retrieved. Time elapsed: 47.47010135650635.\n",
      "Processing claim 6... Progress: 7 / 10\n",
      "Obtained 1392 documents from 1392 urls.\n",
      "Top 10 retrieved. Time elapsed: 42.5123815536499.\n",
      "Processing claim 7... Progress: 8 / 10\n",
      "Obtained 1092 documents from 1092 urls.\n",
      "Top 10 retrieved. Time elapsed: 45.8164598941803.\n",
      "Processing claim 8... Progress: 9 / 10\n",
      "Obtained 1026 documents from 1026 urls.\n",
      "Top 10 retrieved. Time elapsed: 24.790496826171875.\n",
      "Processing claim 9... Progress: 10 / 10\n",
      "Obtained 963 documents from 963 urls.\n",
      "Top 10 retrieved. Time elapsed: 33.23857831954956.\n"
     ]
    }
   ],
   "source": [
    "from aic_averitec.src.reranking.rerank_sentences import get_top_k_sentences_nn\n",
    "%reload_ext autoreload\n",
    "\n",
    "get_top_k_sentences_nn(knowledge_store_dir=str(os.path.join(DATASTORE_PATH, 'output_dev')), claim_file=DEV_PATH, json_output=\"./aic_averitec/data_store/dev_top_k_nn.json\", rerank_model = \"mixedbread-ai/mxbai-rerank-xsmall-v1\", end=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate questions-answer pair for the top sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aic_averitec.src.reranking.question_generation_top_sentences import generate_questions_top_k #question generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP_K_TARGET_KNOWLEDGE = \"./aic_averitec/data_store/dev_top_k.json\"\n",
    "TOP_K_TARGET_KNOWLEDGE = str(os.path.join(DATASTORE_PATH, 'dev_top_k_sentences.json'))\n",
    "\n",
    "generate_questions_top_k(reference_corpus=str(os.path.join(DATASET_PATH, \"train.json\")), target_file=DEV_PATH, top_k_target_knowledge=TOP_K_TARGET_KNOWLEDGE, output_questions=\"./aic_averitec/data_store/dev_top_k_qa.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rerank the QA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: typing_extensions in /mnt/appl/software/typing-extensions/4.3.0-GCCcore-11.3.0/lib/python3.10/site-packages (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aic_averitec.src.reranking.rerank_questions import rerank_qa_pairs #typing extensions dependency!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K_QA_FILE = str(os.path.join(DATASTORE_PATH, 'dev_top_k_qa.json'))\n",
    "# TOP_K_QA_FILE = \"./aic_averitec/data_store/dev_top_k_qa.json\"\n",
    "\n",
    "rerank_qa_pairs(top_k_qa_file=TOP_K_QA_FILE, output_file=\"./aic_averitec/data_store/dev_top_3_rerank_qa.json\", best_checkpoint=str(os.path.join(MODELS_PATH, \"bert_dual_encoder.ckpt\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Veracity prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#convert file from json to jsonl\n",
    "with open(\"aic_averitec/data_store/dev500_questions.json\", \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "#write to jsonl\n",
    "with open(\"aic_averitec/data_store/dev500_questions.jsonl\", \"w\") as f:\n",
    "    for item in json_data:\n",
    "        json.dump(item, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlynatom/venvs/py3.10.4/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.6.3 to v2.3.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../mnt/data/factcheck/averitec-data/pretrained_models/bert_veracity.ckpt`\n",
      "100%|██████████| 500/500 [00:11<00:00, 42.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from aic_averitec.src.prediction.veracity_prediction import veracity_prediction #veracity prediction\n",
    "\n",
    "#CLAIM_WITH_EVIDENCE_FILE = \"./aic_averitec/data_store/dev_top_3_rerank_qa.json\"\n",
    "#CLAIM_WITH_EVIDENCE_FILE = str(os.path.join(DATASTORE_PATH, 'dev_top_3_rerank_qa.json'))\n",
    "CLAIM_WITH_EVIDENCE_FILE = \"aic_averitec/data_store/dev500_questions.jsonl\"\n",
    "\n",
    "veracity_prediction(claim_with_evidence_file=CLAIM_WITH_EVIDENCE_FILE, output_file=\"./aic_averitec/data_store/dev_veracity_prediction.json\", best_checkpoint=str(os.path.join(MODELS_PATH, \"bert_veracity.ckpt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venvs/py3.10.4/lib/python3.10/site-packages (4.29.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in ./venvs/py3.10.4/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./venvs/py3.10.4/lib/python3.10/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /mnt/appl/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/appl/software/PyYAML/6.0-GCCcore-11.3.0/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venvs/py3.10.4/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in ./venvs/py3.10.4/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venvs/py3.10.4/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venvs/py3.10.4/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venvs/py3.10.4/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venvs/py3.10.4/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venvs/py3.10.4/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venvs/py3.10.4/lib/python3.10/site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venvs/py3.10.4/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Using cached transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/home/mlynatom/venvs/py3.10.4/lib/python3.10/site-packages/~-kenizers.libs'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/home/mlynatom/venvs/py3.10.4/lib/python3.10/site-packages/~%kenizers'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.29.2\n",
      "    Uninstalling transformers-4.29.2:\n",
      "      Successfully uninstalled transformers-4.29.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyreft 0.0.5 requires numpy>=1.26.4, but you have numpy 1.22.3 which is incompatible.\n",
      "pyreft 0.0.5 requires protobuf>=3.20.0, but you have protobuf 3.19.4 which is incompatible.\n",
      "pyvene 0.1.1 requires huggingface-hub==0.20.3, but you have huggingface-hub 0.23.4 which is incompatible.\n",
      "pyvene 0.1.1 requires numpy>=1.23.5, but you have numpy 1.22.3 which is incompatible.\n",
      "pyvene 0.1.1 requires protobuf>=3.20.0, but you have protobuf 3.19.4 which is incompatible.\n",
      "vllm 0.2.7 requires pydantic==1.10.13, but you have pydantic 2.7.0 which is incompatible.\n",
      "vllm 0.2.7 requires torch==2.1.2, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.19.1 transformers-4.42.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:22<00:00, 21.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from aic_averitec.src.prediction.veracity_prediction import veracity_prediction_v2\n",
    "\n",
    "veracity_prediction_v2(claim_with_evidence_file=\"aic_averitec/data_store/dev500_questions.jsonl\", output_file=\"aic_averitec/data_store/dev500_veracity_prediction.json\", nei_new_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da58d9e7737e4740995247f1f0ce4e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.3 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 500/500 [06:20<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from aic_averitec.src.prediction.veracity_prediction import veracity_prediction_4concat\n",
    "#model = \"models/averitec/nli_4concat/deberta-v3-large/checkpoint-576\" #best deberta\n",
    "model = \"models/averitec/nli_4concat/mistral-7B-v0.3/checkpoint-15340\" #load with peft! TODO\n",
    "veracity_prediction_4concat(claim_with_evidence_file=\"aic_averitec/data_store/dev500_questions.jsonl\", output_file=\"aic_averitec/data_store/dev500_veracity_prediction_4concat_mistral.json\", best_checkpoint=model, peft=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Veracity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mlynatom/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aic_averitec.src.prediction.evaluate_veracity import evaluate_veracity #veracity evaluatio\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question-only score (HU-meteor):             0.4482806389034514\n",
      "Question-answer score (HU-meteor):           0.272736519646492\n",
      "====================\n",
      "Veracity F1 scores:\n",
      " * Supported:                                0.6768060836501901\n",
      " * Refuted:                                  0.7944732297063903\n",
      " * Not Enough Evidence:                      0.11538461538461539\n",
      " * Conflicting Evidence/Cherrypicking:       0.2075471698113208\n",
      " * macro:                                    0.44855277463812915\n",
      " * acc:                                      0.666\n",
      "--------------------\n",
      "AVeriTeC scores:\n",
      " * Veracity scores (meteor @ 0.1):           0.646\n",
      " * Veracity scores (meteor @ 0.2):           0.484\n",
      " * Veracity scores (meteor @ 0.25):          0.342\n",
      " * Veracity scores (meteor @ 0.3):           0.234\n",
      " * Veracity scores (meteor @ 0.4):           0.088\n",
      " * Veracity scores (meteor @ 0.5):           0.04\n",
      "--------------------\n",
      "AVeriTeC scores by type @ 0.25:\n",
      " * Veracity scores (Event/Property Claim):   0.18362806040517682\n",
      " * Veracity scores (Position Statement):     0.220755019960999\n",
      " * Veracity scores (Causal Claim):           0.150556101994713\n",
      " * Veracity scores (Numerical Claim):        0.20530835082213306\n",
      " * Veracity scores (Quote Verification):     0.15591383933272546\n"
     ]
    }
   ],
   "source": [
    "#PREDICTION_FILE = str(os.path.join(DATASTORE_PATH, 'dev_veracity_prediction.json'))\n",
    "#PREDICTION_FILE = \"aic_averitec/data_store/dev500_veracity_prediction.json\"\n",
    "#PREDICTION_FILE = \"./aic_averitec/data_store/dev_veracity_prediction.json\"\n",
    "#PREDICTION_FILE = \"aic_averitec/data_store/dev500_veracity_prediction_4concat.json\"\n",
    "PREDICTION_FILE = \"aic_averitec/data_store/dev500_veracity_prediction_4concat_mistral.json\"\n",
    "\n",
    "evaluate_veracity(prediction_file=PREDICTION_FILE, label_file=DEV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mistral 4concat\n",
    "\n",
    "Question-only score (HU-meteor):             0.4482806389034514\n",
    "Question-answer score (HU-meteor):           0.272736519646492\n",
    "====================\n",
    "Veracity F1 scores:\n",
    " * Supported:                                0.6768060836501901\n",
    " * Refuted:                                  0.7944732297063903\n",
    " * Not Enough Evidence:                      0.11538461538461539\n",
    " * Conflicting Evidence/Cherrypicking:       0.2075471698113208\n",
    " * macro:                                    0.44855277463812915\n",
    " * acc:                                      0.666\n",
    "--------------------\n",
    "AVeriTeC scores:\n",
    " * Veracity scores (meteor @ 0.1):           0.646\n",
    " * Veracity scores (meteor @ 0.2):           0.484\n",
    " * Veracity scores (meteor @ 0.25):          0.342\n",
    " * Veracity scores (meteor @ 0.3):           0.234\n",
    " * Veracity scores (meteor @ 0.4):           0.088\n",
    " * Veracity scores (meteor @ 0.5):           0.04\n",
    "--------------------\n",
    "AVeriTeC scores by type @ 0.25:\n",
    " * Veracity scores (Event/Property Claim):   0.18362806040517682\n",
    " * Veracity scores (Position Statement):     0.220755019960999\n",
    " * Veracity scores (Causal Claim):           0.150556101994713\n",
    " * Veracity scores (Numerical Claim):        0.20530835082213306\n",
    " * Veracity scores (Quote Verification):     0.15591383933272546"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dev500 deberta 4concat\n",
    "\n",
    "Question-only score (HU-meteor):             0.4482806389034514\n",
    "Question-answer score (HU-meteor):           0.272736519646492\n",
    "====================\n",
    "Veracity F1 scores:\n",
    " * Supported:                                0.6987951807228915\n",
    " * Refuted:                                  0.8227194492254732\n",
    " * Not Enough Evidence:                      0.13043478260869562\n",
    " * Conflicting Evidence/Cherrypicking:       0.19354838709677416\n",
    " * macro:                                    0.46137444991345866\n",
    " * acc:                                      0.682\n",
    "--------------------\n",
    "AVeriTeC scores:\n",
    " * Veracity scores (meteor @ 0.1):           0.664\n",
    " * Veracity scores (meteor @ 0.2):           0.502\n",
    " * Veracity scores (meteor @ 0.25):          0.358\n",
    " * Veracity scores (meteor @ 0.3):           0.246\n",
    " * Veracity scores (meteor @ 0.4):           0.09\n",
    " * Veracity scores (meteor @ 0.5):           0.038\n",
    "--------------------\n",
    "AVeriTeC scores by type @ 0.25:\n",
    " * Veracity scores (Event/Property Claim):   0.18362806040517682\n",
    " * Veracity scores (Position Statement):     0.220755019960999\n",
    " * Veracity scores (Causal Claim):           0.150556101994713\n",
    " * Veracity scores (Numerical Claim):        0.20530835082213306\n",
    " * Veracity scores (Quote Verification):     0.15591383933272546"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### original NLI\n",
    "\n",
    "Question-only score (HU-meteor):             0.4482806389034514\n",
    "Question-answer score (HU-meteor):           0.272736519646492\n",
    "====================\n",
    "Veracity F1 scores:\n",
    " * Supported:                                0.41884816753926696\n",
    " * Refuted:                                  0.601980198019802\n",
    " * Not Enough Evidence:                      0.125\n",
    " * Conflicting Evidence/Cherrypicking:       0.18333333333333335\n",
    " * macro:                                    0.33229042472310055\n",
    " * acc:                                      0.436\n",
    "--------------------\n",
    "AVeriTeC scores:\n",
    " * Veracity scores (meteor @ 0.1):           0.422\n",
    " * Veracity scores (meteor @ 0.2):           0.32\n",
    " * Veracity scores (meteor @ 0.25):          0.234\n",
    " * Veracity scores (meteor @ 0.3):           0.164\n",
    " * Veracity scores (meteor @ 0.4):           0.084\n",
    " * Veracity scores (meteor @ 0.5):           0.038\n",
    "--------------------\n",
    "AVeriTeC scores by type @ 0.25:\n",
    " * Veracity scores (Event/Property Claim):   0.18362806040517682\n",
    " * Veracity scores (Position Statement):     0.220755019960999\n",
    " * Veracity scores (Causal Claim):           0.150556101994713\n",
    " * Veracity scores (Numerical Claim):        0.20530835082213306\n",
    " * Veracity scores (Quote Verification):     0.15591383933272546"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "\n",
    "Question-only score (HU-meteor):             0.24041210604919014\n",
    "Question-answer score (HU-meteor):           0.18547341231661782\n",
    "====================\n",
    "Veracity F1 scores:\n",
    " * Supported:                                0.4372093023255814\n",
    " * Refuted:                                  0.7138157894736843\n",
    " * Not Enough Evidence:                      0.0\n",
    " * Conflicting Evidence/Cherrypicking:       0.13333333333333333\n",
    " * macro:                                    0.32108960628314975\n",
    " * acc:                                      0.546\n",
    "--------------------\n",
    "AVeriTeC scores:\n",
    " * Veracity scores (meteor @ 0.1):           0.452\n",
    " * Veracity scores (meteor @ 0.2):           0.186\n",
    " * Veracity scores (meteor @ 0.25):          0.092\n",
    " * Veracity scores (meteor @ 0.3):           0.05\n",
    " * Veracity scores (meteor @ 0.4):           0.012\n",
    " * Veracity scores (meteor @ 0.5):           0.002\n",
    "--------------------\n",
    "AVeriTeC scores by type @ 0.25:\n",
    " * Veracity scores (Event/Property Claim):   0.05979024836242316\n",
    " * Veracity scores (Position Statement):     0.07307235683162566\n",
    " * Veracity scores (Causal Claim):           0.056993318700669676\n",
    " * Veracity scores (Numerical Claim):        0.070268725336461\n",
    " * Veracity scores (Quote Verification):     0.06266365543813311"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
