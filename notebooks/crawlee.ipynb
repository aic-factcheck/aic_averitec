{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.5\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: crawlee in ./venvs/py3.10.4/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: aiofiles<24.0.0,>=23.2.1 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee) (23.2.1)\n",
      "Requirement already satisfied: aioshutil<2.0,>=1.3 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee) (1.5)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.6 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee) (0.4.6)\n",
      "Requirement already satisfied: cookiecutter<3.0.0,>=2.6.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee) (2.6.0)\n",
      "Requirement already satisfied: docutils<0.22.0,>=0.21.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee) (0.21.2)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee) (0.2.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpx[brotli]<0.28.0,>=0.27.0->crawlee) (0.27.0)\n",
      "Requirement already satisfied: inquirer<4.0.0,>=3.3.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee) (3.3.0)\n",
      "Requirement already satisfied: more_itertools<11.0.0,>=10.2.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee) (10.3.0)\n",
      "Requirement already satisfied: psutil<7.0.0,>=6.0.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee) (6.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee) (2.7.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.2.1 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee) (2.3.4)\n",
      "Requirement already satisfied: pyee<12.0.0,>=11.1.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee) (11.1.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee) (2.9.0.post0)\n",
      "Requirement already satisfied: sortedcollections<3.0.0,>=2.1.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee) (2.1.0)\n",
      "Requirement already satisfied: tldextract<6.0.0,>=5.1.2 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee) (5.1.2)\n",
      "Requirement already satisfied: typer<0.13.0,>=0.12.3 in ./venvs/py3.10.4/lib/python3.10/site-packages (from typer[all]<0.13.0,>=0.12.3->crawlee) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in /mnt/appl/software/typing-extensions/4.3.0-GCCcore-11.3.0/lib/python3.10/site-packages (from crawlee) (4.3.0)\n",
      "Requirement already satisfied: binaryornot>=0.4.4 in ./venvs/py3.10.4/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee) (0.4.4)\n",
      "Requirement already satisfied: Jinja2<4.0.0,>=2.7 in ./venvs/py3.10.4/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee) (3.1.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee) (8.1.7)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /mnt/appl/software/PyYAML/6.0-GCCcore-11.3.0/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee) (6.0)\n",
      "Requirement already satisfied: python-slugify>=4.0.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee) (8.0.4)\n",
      "Requirement already satisfied: requests>=2.23.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee) (2.31.0)\n",
      "Requirement already satisfied: arrow in ./venvs/py3.10.4/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee) (1.3.0)\n",
      "Requirement already satisfied: rich in ./venvs/py3.10.4/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee) (13.7.1)\n",
      "Requirement already satisfied: anyio in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->httpx[brotli]<0.28.0,>=0.27.0->crawlee) (4.0.0)\n",
      "Requirement already satisfied: certifi in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->httpx[brotli]<0.28.0,>=0.27.0->crawlee) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->httpx[brotli]<0.28.0,>=0.27.0->crawlee) (1.0.5)\n",
      "Requirement already satisfied: idna in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->httpx[brotli]<0.28.0,>=0.27.0->crawlee) (3.4)\n",
      "Requirement already satisfied: sniffio in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->httpx[brotli]<0.28.0,>=0.27.0->crawlee) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->httpx[brotli]<0.28.0,>=0.27.0->crawlee) (0.14.0)\n",
      "Requirement already satisfied: brotli in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpx[brotli]<0.28.0,>=0.27.0->crawlee) (1.1.0)\n",
      "Requirement already satisfied: blessed>=1.19.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from inquirer<4.0.0,>=3.3.0->crawlee) (1.20.0)\n",
      "Requirement already satisfied: editor>=1.6.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from inquirer<4.0.0,>=3.3.0->crawlee) (1.6.6)\n",
      "Requirement already satisfied: readchar>=3.0.6 in ./venvs/py3.10.4/lib/python3.10/site-packages (from inquirer<4.0.0,>=3.3.0->crawlee) (4.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.6.3->crawlee) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in ./venvs/py3.10.4/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.6.3->crawlee) (2.18.1)\n",
      "Collecting typing-extensions<5.0.0,>=4.1.0 (from crawlee)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.2.1->crawlee) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venvs/py3.10.4/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.9.0->crawlee) (1.16.0)\n",
      "Requirement already satisfied: sortedcontainers in ./venvs/py3.10.4/lib/python3.10/site-packages (from sortedcollections<3.0.0,>=2.1.0->crawlee) (2.4.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in ./venvs/py3.10.4/lib/python3.10/site-packages (from tldextract<6.0.0,>=5.1.2->crawlee) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in ./venvs/py3.10.4/lib/python3.10/site-packages (from tldextract<6.0.0,>=5.1.2->crawlee) (3.12.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from typer<0.13.0,>=0.12.3->typer[all]<0.13.0,>=0.12.3->crawlee) (1.5.4)\n",
      "\u001b[33mWARNING: typer 0.12.3 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: chardet>=3.0.2 in ./venvs/py3.10.4/lib/python3.10/site-packages (from binaryornot>=0.4.4->cookiecutter<3.0.0,>=2.6.0->crawlee) (5.2.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in ./venvs/py3.10.4/lib/python3.10/site-packages (from blessed>=1.19.0->inquirer<4.0.0,>=3.3.0->crawlee) (0.2.8)\n",
      "Requirement already satisfied: runs in ./venvs/py3.10.4/lib/python3.10/site-packages (from editor>=1.6.0->inquirer<4.0.0,>=3.3.0->crawlee) (1.2.2)\n",
      "Requirement already satisfied: xmod in ./venvs/py3.10.4/lib/python3.10/site-packages (from editor>=1.6.0->inquirer<4.0.0,>=3.3.0->crawlee) (1.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from Jinja2<4.0.0,>=2.7->cookiecutter<3.0.0,>=2.6.0->crawlee) (2.1.3)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in ./venvs/py3.10.4/lib/python3.10/site-packages (from python-slugify>=4.0.0->cookiecutter<3.0.0,>=2.6.0->crawlee) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venvs/py3.10.4/lib/python3.10/site-packages (from requests>=2.23.0->cookiecutter<3.0.0,>=2.6.0->crawlee) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venvs/py3.10.4/lib/python3.10/site-packages (from requests>=2.23.0->cookiecutter<3.0.0,>=2.6.0->crawlee) (2.0.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from rich->cookiecutter<3.0.0,>=2.6.0->crawlee) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from rich->cookiecutter<3.0.0,>=2.6.0->crawlee) (2.16.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venvs/py3.10.4/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->httpx[brotli]<0.28.0,>=0.27.0->crawlee) (1.1.3)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in ./venvs/py3.10.4/lib/python3.10/site-packages (from arrow->cookiecutter<3.0.0,>=2.6.0->crawlee) (2.8.19.14)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venvs/py3.10.4/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->cookiecutter<3.0.0,>=2.6.0->crawlee) (0.1.2)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Not uninstalling typing-extensions at /mnt/appl/software/typing-extensions/4.3.0-GCCcore-11.3.0/lib/python3.10/site-packages, outside environment /home/mlynatom/venvs/py3.10.4\n",
      "    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.9.1 requires flatbuffers<2,>=1.12, but you have flatbuffers 2.0 which is incompatible.\n",
      "pyreft 0.0.5 requires numpy>=1.26.4, but you have numpy 1.22.3 which is incompatible.\n",
      "pyreft 0.0.5 requires protobuf>=3.20.0, but you have protobuf 3.19.4 which is incompatible.\n",
      "pyvene 0.1.1 requires huggingface-hub==0.20.3, but you have huggingface-hub 0.23.4 which is incompatible.\n",
      "pyvene 0.1.1 requires numpy>=1.23.5, but you have numpy 1.22.3 which is incompatible.\n",
      "pyvene 0.1.1 requires protobuf>=3.20.0, but you have protobuf 3.19.4 which is incompatible.\n",
      "quanto 0.1.0 requires torch>=2.2.0, but you have torch 2.0.1 which is incompatible.\n",
      "vllm 0.2.7 requires pydantic==1.10.13, but you have pydantic 2.7.0 which is incompatible.\n",
      "vllm 0.2.7 requires torch==2.1.2, but you have torch 2.0.1 which is incompatible.\n",
      "xformers 0.0.23.post1 requires torch==2.1.2, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed typing-extensions-4.11.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install crawlee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: crawlee[beautifulsoup] in ./venvs/py3.10.4/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: aiofiles<24.0.0,>=23.2.1 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (23.2.1)\n",
      "Requirement already satisfied: aioshutil<2.0,>=1.3 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (1.5)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.6 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (0.4.6)\n",
      "Requirement already satisfied: cookiecutter<3.0.0,>=2.6.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (2.6.0)\n",
      "Requirement already satisfied: docutils<0.22.0,>=0.21.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (0.21.2)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (0.2.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpx[brotli]<0.28.0,>=0.27.0->crawlee[beautifulsoup]) (0.27.0)\n",
      "Requirement already satisfied: inquirer<4.0.0,>=3.3.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (3.3.0)\n",
      "Requirement already satisfied: more_itertools<11.0.0,>=10.2.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (10.3.0)\n",
      "Requirement already satisfied: psutil<7.0.0,>=6.0.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (6.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (2.7.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.2.1 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (2.3.4)\n",
      "Requirement already satisfied: pyee<12.0.0,>=11.1.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (11.1.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (2.9.0.post0)\n",
      "Requirement already satisfied: sortedcollections<3.0.0,>=2.1.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (2.1.0)\n",
      "Requirement already satisfied: tldextract<6.0.0,>=5.1.2 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (5.1.2)\n",
      "Requirement already satisfied: typer<0.13.0,>=0.12.3 in ./venvs/py3.10.4/lib/python3.10/site-packages (from typer[all]<0.13.0,>=0.12.3->crawlee[beautifulsoup]) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in /mnt/appl/software/typing-extensions/4.3.0-GCCcore-11.3.0/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (4.3.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (4.12.3)\n",
      "Requirement already satisfied: html5lib<2.0,>=1.1 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (1.1)\n",
      "Requirement already satisfied: lxml<6.0.0,>=5.2.1 in ./venvs/py3.10.4/lib/python3.10/site-packages (from crawlee[beautifulsoup]) (5.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venvs/py3.10.4/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->crawlee[beautifulsoup]) (2.5)\n",
      "Requirement already satisfied: binaryornot>=0.4.4 in ./venvs/py3.10.4/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (0.4.4)\n",
      "Requirement already satisfied: Jinja2<4.0.0,>=2.7 in ./venvs/py3.10.4/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (3.1.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (8.1.7)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /mnt/appl/software/PyYAML/6.0-GCCcore-11.3.0/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (6.0)\n",
      "Requirement already satisfied: python-slugify>=4.0.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (8.0.4)\n",
      "Requirement already satisfied: requests>=2.23.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (2.31.0)\n",
      "Requirement already satisfied: arrow in ./venvs/py3.10.4/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (1.3.0)\n",
      "Requirement already satisfied: rich in ./venvs/py3.10.4/lib/python3.10/site-packages (from cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (13.7.1)\n",
      "Requirement already satisfied: six>=1.9 in ./venvs/py3.10.4/lib/python3.10/site-packages (from html5lib<2.0,>=1.1->crawlee[beautifulsoup]) (1.16.0)\n",
      "Requirement already satisfied: webencodings in ./venvs/py3.10.4/lib/python3.10/site-packages (from html5lib<2.0,>=1.1->crawlee[beautifulsoup]) (0.5.1)\n",
      "Requirement already satisfied: anyio in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->httpx[brotli]<0.28.0,>=0.27.0->crawlee[beautifulsoup]) (4.0.0)\n",
      "Requirement already satisfied: certifi in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->httpx[brotli]<0.28.0,>=0.27.0->crawlee[beautifulsoup]) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->httpx[brotli]<0.28.0,>=0.27.0->crawlee[beautifulsoup]) (1.0.5)\n",
      "Requirement already satisfied: idna in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->httpx[brotli]<0.28.0,>=0.27.0->crawlee[beautifulsoup]) (3.4)\n",
      "Requirement already satisfied: sniffio in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->httpx[brotli]<0.28.0,>=0.27.0->crawlee[beautifulsoup]) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->httpx[brotli]<0.28.0,>=0.27.0->crawlee[beautifulsoup]) (0.14.0)\n",
      "Requirement already satisfied: brotli in ./venvs/py3.10.4/lib/python3.10/site-packages (from httpx[brotli]<0.28.0,>=0.27.0->crawlee[beautifulsoup]) (1.1.0)\n",
      "Requirement already satisfied: blessed>=1.19.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from inquirer<4.0.0,>=3.3.0->crawlee[beautifulsoup]) (1.20.0)\n",
      "Requirement already satisfied: editor>=1.6.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from inquirer<4.0.0,>=3.3.0->crawlee[beautifulsoup]) (1.6.6)\n",
      "Requirement already satisfied: readchar>=3.0.6 in ./venvs/py3.10.4/lib/python3.10/site-packages (from inquirer<4.0.0,>=3.3.0->crawlee[beautifulsoup]) (4.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.6.3->crawlee[beautifulsoup]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in ./venvs/py3.10.4/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.6.3->crawlee[beautifulsoup]) (2.18.1)\n",
      "Collecting typing-extensions<5.0.0,>=4.1.0 (from crawlee[beautifulsoup])\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.2.1->crawlee[beautifulsoup]) (1.0.0)\n",
      "Requirement already satisfied: sortedcontainers in ./venvs/py3.10.4/lib/python3.10/site-packages (from sortedcollections<3.0.0,>=2.1.0->crawlee[beautifulsoup]) (2.4.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in ./venvs/py3.10.4/lib/python3.10/site-packages (from tldextract<6.0.0,>=5.1.2->crawlee[beautifulsoup]) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in ./venvs/py3.10.4/lib/python3.10/site-packages (from tldextract<6.0.0,>=5.1.2->crawlee[beautifulsoup]) (3.12.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from typer<0.13.0,>=0.12.3->typer[all]<0.13.0,>=0.12.3->crawlee[beautifulsoup]) (1.5.4)\n",
      "\u001b[33mWARNING: typer 0.12.3 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: chardet>=3.0.2 in ./venvs/py3.10.4/lib/python3.10/site-packages (from binaryornot>=0.4.4->cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (5.2.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in ./venvs/py3.10.4/lib/python3.10/site-packages (from blessed>=1.19.0->inquirer<4.0.0,>=3.3.0->crawlee[beautifulsoup]) (0.2.8)\n",
      "Requirement already satisfied: runs in ./venvs/py3.10.4/lib/python3.10/site-packages (from editor>=1.6.0->inquirer<4.0.0,>=3.3.0->crawlee[beautifulsoup]) (1.2.2)\n",
      "Requirement already satisfied: xmod in ./venvs/py3.10.4/lib/python3.10/site-packages (from editor>=1.6.0->inquirer<4.0.0,>=3.3.0->crawlee[beautifulsoup]) (1.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from Jinja2<4.0.0,>=2.7->cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (2.1.3)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in ./venvs/py3.10.4/lib/python3.10/site-packages (from python-slugify>=4.0.0->cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venvs/py3.10.4/lib/python3.10/site-packages (from requests>=2.23.0->cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venvs/py3.10.4/lib/python3.10/site-packages (from requests>=2.23.0->cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (2.0.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from rich->cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venvs/py3.10.4/lib/python3.10/site-packages (from rich->cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (2.16.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venvs/py3.10.4/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->httpx[brotli]<0.28.0,>=0.27.0->crawlee[beautifulsoup]) (1.1.3)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in ./venvs/py3.10.4/lib/python3.10/site-packages (from arrow->cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (2.8.19.14)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venvs/py3.10.4/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->cookiecutter<3.0.0,>=2.6.0->crawlee[beautifulsoup]) (0.1.2)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Not uninstalling typing-extensions at /mnt/appl/software/typing-extensions/4.3.0-GCCcore-11.3.0/lib/python3.10/site-packages, outside environment /home/mlynatom/venvs/py3.10.4\n",
      "    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.9.1 requires flatbuffers<2,>=1.12, but you have flatbuffers 2.0 which is incompatible.\n",
      "pyreft 0.0.5 requires numpy>=1.26.4, but you have numpy 1.22.3 which is incompatible.\n",
      "pyreft 0.0.5 requires protobuf>=3.20.0, but you have protobuf 3.19.4 which is incompatible.\n",
      "pyvene 0.1.1 requires huggingface-hub==0.20.3, but you have huggingface-hub 0.23.4 which is incompatible.\n",
      "pyvene 0.1.1 requires numpy>=1.23.5, but you have numpy 1.22.3 which is incompatible.\n",
      "pyvene 0.1.1 requires protobuf>=3.20.0, but you have protobuf 3.19.4 which is incompatible.\n",
      "quanto 0.1.0 requires torch>=2.2.0, but you have torch 2.0.1 which is incompatible.\n",
      "vllm 0.2.7 requires pydantic==1.10.13, but you have pydantic 2.7.0 which is incompatible.\n",
      "vllm 0.2.7 requires torch==2.1.2, but you have torch 2.0.1 which is incompatible.\n",
      "xformers 0.0.23.post1 requires torch==2.1.2, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed typing-extensions-4.11.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install crawlee[beautifulsoup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/aic_averitec/src/utils/crawl.py:28\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m crawler\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://crawlee.dev\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/appl/software/Python/3.11.5-GCCcore-13.2.0/lib/python3.11/asyncio/runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "%run aic_averitec/src/utils/crawl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[crawlee.statistics.statistics]\u001b[0m \u001b[32mINFO \u001b[0m crawlee.beautifulsoup_crawler.beautifulsoup_crawler request statistics {\n",
      "        \"requests_finished\": 0,\n",
      "        \"requests_failed\": 0,\n",
      "        \"retry_histogram\": [\n",
      "          0\n",
      "        ],\n",
      "        \"request_avg_failed_duration\": null,\n",
      "        \"request_avg_finished_duration\": null,\n",
      "        \"requests_finished_per_minute\": 0,\n",
      "        \"requests_failed_per_minute\": 0,\n",
      "        \"request_total_duration\": 0.0,\n",
      "        \"requests_total\": 0,\n",
      "        \"crawler_runtime\": 0.014669\n",
      "      }\n",
      "\u001b[90m[crawlee.autoscaling.autoscaled_pool]\u001b[0m \u001b[32mINFO \u001b[0m current_concurrency = 0; desired_concurrency = 2; cpu = 0; mem = 0; event_loop = 0.0; client_info = 0.0\n",
      "/mnt/appl/software/BeautifulSoup/4.12.2-GCCcore-13.2.0/lib/python3.11/site-packages/bs4/builder/__init__.py:314: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  for attr in list(attrs.keys()):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "\u001b[90m[crawlee.beautifulsoup_crawler.beautifulsoup_crawler]\u001b[0m \u001b[32mINFO \u001b[0m The crawler has reached its limit of 50 requests per crawl. All ongoing requests have now completed. Total requests processed: 50. The crawler will now shut down.\n",
      "\u001b[90m[crawlee.beautifulsoup_crawler.beautifulsoup_crawler]\u001b[0m \u001b[32mINFO \u001b[0m is_finished: False\n",
      "\u001b[90m[crawlee.autoscaling.autoscaled_pool]\u001b[0m \u001b[32mINFO \u001b[0m Waiting for remaining tasks to finish\n",
      "\u001b[90m[crawlee.beautifulsoup_crawler.beautifulsoup_crawler]\u001b[0m \u001b[32mINFO \u001b[0m Final request statistics: {\n",
      "        \"requests_finished\": 52,\n",
      "        \"requests_failed\": 0,\n",
      "        \"retry_histogram\": [\n",
      "          52\n",
      "        ],\n",
      "        \"request_avg_failed_duration\": null,\n",
      "        \"request_avg_finished_duration\": 0.841775,\n",
      "        \"requests_finished_per_minute\": 168,\n",
      "        \"requests_failed_per_minute\": 0,\n",
      "        \"request_total_duration\": 43.772295,\n",
      "        \"requests_total\": 52,\n",
      "        \"crawler_runtime\": 18.5632\n",
      "      }\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from crawlee.beautifulsoup_crawler import BeautifulSoupCrawler, BeautifulSoupCrawlingContext\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    # BeautifulSoupCrawler crawls the web using HTTP requests and parses HTML using the BeautifulSoup library.\n",
    "    crawler = BeautifulSoupCrawler(max_requests_per_crawl=50)\n",
    "\n",
    "    # Define a request handler to process each crawled page and attach it to the crawler using a decorator.\n",
    "    @crawler.router.default_handler\n",
    "    async def request_handler(context: BeautifulSoupCrawlingContext) -> None:\n",
    "        # Extract relevant data from the page context.\n",
    "        data = {\n",
    "            'url': context.request.url,\n",
    "            'title': context.soup.title.string if context.soup.title else None,\n",
    "        }\n",
    "        # Store the extracted data.\n",
    "        await context.push_data(data)\n",
    "        # Extract links from the current page and add them to the crawling queue.\n",
    "        await context.enqueue_links()\n",
    "\n",
    "    # Add first URL to the queue and start the crawl.\n",
    "    await crawler.run(['https://crawlee.dev'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    await(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mlynatom\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"url\": \"https://crawlee.dev\",\n",
      "  \"title\": \"Crawlee · Build reliable crawlers. Fast.\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat ./storage/datasets/default/000000001.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /mnt/appl/software/BeautifulSoup/4.12.2-GCCcore-13.2.0/lib/python3.11/site-packages (from newspaper3k) (4.12.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /mnt/appl/software/Pillow/10.2.0-GCCcore-13.2.0/lib/python3.11/site-packages (from newspaper3k) (10.2.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /mnt/appl/software/PyYAML/6.0.1-GCCcore-13.2.0/lib/python3.11/site-packages (from newspaper3k) (6.0.1)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /mnt/appl/software/lxml/4.9.3-GCCcore-13.2.0/lib/python3.11/site-packages (from newspaper3k) (4.9.3)\n",
      "Collecting nltk>=3.2.1 (from newspaper3k)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: requests>=2.10.0 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from newspaper3k) (2.31.0)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in ./venvs/crawlee/lib/python3.11/site-packages (from newspaper3k) (5.1.2)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from newspaper3k) (2.8.2)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /mnt/appl/software/BeautifulSoup/4.12.2-GCCcore-13.2.0/lib/python3.11/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
      "Requirement already satisfied: six in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
      "Requirement already satisfied: joblib in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from nltk>=3.2.1->newspaper3k) (2023.10.3)\n",
      "Collecting tqdm (from nltk>=3.2.1->newspaper3k)\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from requests>=2.10.0->newspaper3k) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from requests>=2.10.0->newspaper3k) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from requests>=2.10.0->newspaper3k) (2023.7.22)\n",
      "Requirement already satisfied: requests-file>=1.4 in ./venvs/crawlee/lib/python3.11/site-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from tldextract>=2.0.1->newspaper3k) (3.13.0)\n",
      "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13539 sha256=791ecfb5ecaf525d8e389973ba9d76cfada5b60003f4380157b3a812bec3eb9b\n",
      "  Stored in directory: /home/mlynatom/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
      "  Building wheel for feedfinder2 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3342 sha256=cd133c48f7ddd22fc2361cdd0788a2bbb751d831dc19054d6057e219ac4c6c9c\n",
      "  Stored in directory: /home/mlynatom/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
      "  Building wheel for jieba3k (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=9faf6b3c5847440bdd48544f5373d23da45881f29345a9209af91a8842b737c0\n",
      "  Stored in directory: /home/mlynatom/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
      "  Building wheel for sgmllib3k (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=b3e589bbf5913ea04eb25bea65fb1cf2b031df46db8ca6acfb02e40709bda55f\n",
      "  Stored in directory: /home/mlynatom/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, tqdm, feedparser, cssselect, nltk, feedfinder2, newspaper3k\n",
      "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 nltk-3.8.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tqdm-4.66.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\n"
     ]
    },
    {
     "ename": "ArticleException",
     "evalue": "Article `download()` failed with 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/world/us/republicans-target-biden-blame-over-putins-ukraine-invasion-2022-02-24/ on URL https://www.reuters.com/world/us/republicans-target-biden-blame-over-putins-ukraine-invasion-2022-02-24/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArticleException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m a \u001b[38;5;241m=\u001b[39m Article(url\u001b[38;5;241m=\u001b[39murl, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m     14\u001b[0m a\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[0;32m---> 15\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mtitle)\n",
      "File \u001b[0;32m~/venvs/crawlee/lib/python3.11/site-packages/newspaper/article.py:191\u001b[0m, in \u001b[0;36mArticle.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow_if_not_downloaded_verbose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_parser()\u001b[38;5;241m.\u001b[39mfromstring(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhtml)\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_doc \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc)\n",
      "File \u001b[0;32m~/venvs/crawlee/lib/python3.11/site-packages/newspaper/article.py:531\u001b[0m, in \u001b[0;36mArticle.throw_if_not_downloaded_verbose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou must `download()` an article first!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_state \u001b[38;5;241m==\u001b[39m ArticleDownloadState\u001b[38;5;241m.\u001b[39mFAILED_RESPONSE:\n\u001b[0;32m--> 531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle `download()` failed with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m on URL \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    532\u001b[0m           (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_exception_msg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl))\n",
      "\u001b[0;31mArticleException\u001b[0m: Article `download()` failed with 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/world/us/republicans-target-biden-blame-over-putins-ukraine-invasion-2022-02-24/ on URL https://www.reuters.com/world/us/republicans-target-biden-blame-over-putins-ukraine-invasion-2022-02-24/"
     ]
    }
   ],
   "source": [
    "from newspaper import Article, Config\n",
    "# url = \"https://www.defense.gov/News/News-Stories/Article/Article/3615637/biden-pledges-us-will-not-walk-away-from-ukraine/\"\n",
    "#url = \"https://www.politico.com/news/magazine/2023/12/27/biden-endgame-ukraine-00133211\"\n",
    "url = \"https://www.reuters.com/world/us/republicans-target-biden-blame-over-putins-ukraine-invasion-2022-02-24/\"\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "config.verbose = True\n",
    "print(config.browser_user_agent)\n",
    "a = Article(url=url, config=config)\n",
    "\n",
    "a.download()\n",
    "a.parse()\n",
    "\n",
    "print(a.text)\n",
    "print(a.title)\n",
    "print(a.authors)\n",
    "print(a.publish_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from CCI: {\"message\": \"No Captures found for: https://www.reuters.com/world/biden-announces-new-sanctions-vs-russia-two-years-into-ukraine-war-2024-02-23/\"}\n",
      "No records found for https://www.reuters.com/world/biden-announces-new-sanctions-vs-russia-two-years-into-ukraine-war-2024-02-23/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# Please note: f-strings require Python 3.6+\n",
    "\n",
    "# The URL of the Common Crawl Index server\n",
    "CC_INDEX_SERVER = 'http://index.commoncrawl.org/'\n",
    "\n",
    "# The Common Crawl index you want to query\n",
    "INDEX_NAME = 'CC-MAIN-2024-26'      # Replace with the latest index name\n",
    "\n",
    "# The URL you want to look up in the Common Crawl index\n",
    "target_url = \"https://www.reuters.com/world/biden-announces-new-sanctions-vs-russia-two-years-into-ukraine-war-2024-02-23/\"\n",
    "# Function to search the Common Crawl Index\n",
    "def search_cc_index(url):\n",
    "    encoded_url = quote_plus(url)\n",
    "    index_url = f'{CC_INDEX_SERVER}{INDEX_NAME}-index?url={encoded_url}&output=json'\n",
    "    response = requests.get(index_url)\n",
    "    print(\"Response from CCI:\", response.text)  # Output the response from the server\n",
    "    if response.status_code == 200:\n",
    "        records = response.text.strip().split('\\n')\n",
    "        return [json.loads(record) for record in records]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to fetch the content from Common Crawl\n",
    "def fetch_page_from_cc(records):\n",
    "    for record in records:\n",
    "        offset, length = int(record['offset']), int(record['length'])\n",
    "        prefix = record['filename'].split('/')[0]\n",
    "        s3_url = f'https://data.commoncrawl.org/{record[\"filename\"]}'\n",
    "        response = requests.get(s3_url, headers={'Range': f'bytes={offset}-{offset+length-1}'})\n",
    "        if response.status_code == 206:\n",
    "            # Process the response content if necessary\n",
    "            # For example, you can use warcio to parse the WARC record\n",
    "            return response.content\n",
    "        else:\n",
    "            print(f\"Failed to fetch data: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "# Search the index for the target URL\n",
    "records = search_cc_index(target_url)\n",
    "if records:\n",
    "    print(f\"Found {len(records)} records for {target_url}\")\n",
    "\n",
    "    # Fetch the page content from the first record\n",
    "    content = fetch_page_from_cc(records)\n",
    "    if content:\n",
    "        print(f\"Successfully fetched content for {target_url}\")\n",
    "        # You can now process the 'content' variable as needed\n",
    "else:\n",
    "    print(f\"No records found for {target_url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
