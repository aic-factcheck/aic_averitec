{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               claim    label  \\\n",
      "0  In a letter to Steve Jobs, Sean Connery refuse...  Refuted   \n",
      "1  Trump Administration claimed songwriter Billie...  Refuted   \n",
      "2  Due to Imran Khan's criticism of Macron's comm...  Refuted   \n",
      "3  UNESCO declared Nadar community as the most an...  Refuted   \n",
      "4  Republican Matt Gaetz was part of a company th...  Refuted   \n",
      "\n",
      "   emotional_word_count  \n",
      "0                     0  \n",
      "1                     0  \n",
      "2                     0  \n",
      "3                     1  \n",
      "4                     0  \n",
      "label\n",
      "Conflicting Evidence/Cherrypicking    0.131579\n",
      "Not Enough Evidence                   0.857143\n",
      "Refuted                               0.131148\n",
      "Supported                             0.327869\n",
      "Name: emotional_word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/home/aaronbry/my_env/data/dev.json'\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Define a list of common emotional words\n",
    "#emotional_words = [\"love\", \"hate\", \"wonderful\", \"excess\", \"terrible\", \"fantastic\", \"awful\", \"great\", \"bad\", \"amazing\", \"horrible\", \"excellent\", \"poor\", \"disaster\"]\n",
    "emotional_words = [\n",
    "    'more',\n",
    "    'most',\n",
    "    'higher',\n",
    "    'biggest',\n",
    "    'best',\n",
    "    'cheaper',\n",
    "    'lower',\n",
    "    'fewer',\n",
    "    'less',\n",
    "    'nigeria',\n",
    "    'bigger',\n",
    "    'greatest',\n",
    "    'speaker',\n",
    "    'worse',\n",
    "    'older',\n",
    "    'least',\n",
    "    'saudi',\n",
    "    'lighter',\n",
    "    'fastest',\n",
    "    'wildfire',\n",
    "    'greater',\n",
    "    'highest',\n",
    "    'richest'\n",
    "]\n",
    "\n",
    "# Preprocess the text to make it easier to count the emotional words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Function to count the number of emotional words in each claim\n",
    "def count_emotional_words(tokens, emotional_words):\n",
    "    count = sum(1 for word in tokens if word in emotional_words)\n",
    "    return count\n",
    "\n",
    "# Apply preprocessing and count emotional words\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "data['emotional_word_count'] = data['tokens'].apply(lambda tokens: count_emotional_words(tokens, emotional_words))\n",
    "\n",
    "# Display the DataFrame with the new columns\n",
    "print(data[['claim', 'label', 'emotional_word_count']].head())\n",
    "\n",
    "# Analyze the correlation between emotional word count and the labels\n",
    "emotional_word_count_analysis = data.groupby('label')['emotional_word_count'].mean()\n",
    "print(emotional_word_count_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               claim    label  \\\n",
      "0  In a letter to Steve Jobs, Sean Connery refuse...  Refuted   \n",
      "1  Trump Administration claimed songwriter Billie...  Refuted   \n",
      "2  Due to Imran Khan's criticism of Macron's comm...  Refuted   \n",
      "3  UNESCO declared Nadar community as the most an...  Refuted   \n",
      "4  Republican Matt Gaetz was part of a company th...  Refuted   \n",
      "\n",
      "   emotional_word_count  \n",
      "0                     0  \n",
      "1                     0  \n",
      "2                     0  \n",
      "3                     1  \n",
      "4                     0  \n",
      "label\n",
      "Conflicting Evidence/Cherrypicking    0.131579\n",
      "Not Enough Evidence                   0.857143\n",
      "Refuted                               0.131148\n",
      "Supported                             0.327869\n",
      "Name: emotional_word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/home/aaronbry/my_env/data/dev.json'\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Define the list of common emotional words\n",
    "emotional_words_list = [\n",
    "    'more', 'most', 'higher', 'biggest', 'best', 'cheaper', 'lower', 'fewer',\n",
    "    'less', 'nigeria', 'bigger', 'greatest', 'speaker', 'worse', 'older',\n",
    "    'least', 'saudi', 'lighter', 'fastest', 'wildfire', 'greater', 'highest',\n",
    "    'richest'\n",
    "]\n",
    "\n",
    "# Preprocess the text to make it easier to count the emotional words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Function to count the number of emotional words in each claim\n",
    "def count_emotional_words(tokens, emotional_words):\n",
    "    count = sum(1 for word in tokens if word in emotional_words)\n",
    "    return count\n",
    "\n",
    "# Apply preprocessing and count emotional words\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "data['emotional_word_count'] = data['tokens'].apply(lambda tokens: count_emotional_words(tokens, emotional_words_list))\n",
    "\n",
    "# Display the DataFrame with the new columns\n",
    "print(data[['claim', 'label', 'emotional_word_count']].head())\n",
    "\n",
    "# Analyze the correlation between emotional word count and the labels\n",
    "emotional_word_count_analysis = data.groupby('label')['emotional_word_count'].mean()\n",
    "print(emotional_word_count_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word  Frequency\n",
      "0       more         11\n",
      "1       most          8\n",
      "2     higher          5\n",
      "3    biggest          4\n",
      "4       best          3\n",
      "5    cheaper          3\n",
      "6      lower          3\n",
      "7      fewer          2\n",
      "8       less          2\n",
      "9    nigeria          2\n",
      "10    bigger          1\n",
      "11  greatest          1\n",
      "12   speaker          1\n",
      "13     worse          1\n",
      "14     older          1\n",
      "15     least          1\n",
      "16     saudi          1\n",
      "17   lighter          1\n",
      "18   fastest          1\n",
      "19  wildfire          1\n",
      "20   greater          1\n",
      "21   highest          1\n",
      "22   richest          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "#nltk.download('punkt')\n",
    "# Download necessary NLTK data\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/home/aaronbry/my_env/data/dev.json'\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Preprocess the text to make it easier to extract words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "\n",
    "# Flatten all tokens into a single list\n",
    "all_tokens = [token for tokens in data['tokens'] for token in tokens]\n",
    "\n",
    "# POS tagging to filter adjectives and emotional words\n",
    "tagged_tokens = nltk.pos_tag(all_tokens)\n",
    "adjective_words = [word for word, pos in tagged_tokens if pos in ['JJR', 'JJS', 'RB' 'RBR', 'RBS']]\n",
    "\n",
    "# Frequency distribution of adjectives/emotional words\n",
    "freq_dist = Counter(adjective_words)\n",
    "\n",
    "# Get the 100 most common adjectives/emotional words\n",
    "most_common_adjectives = freq_dist.most_common(100)\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "most_common_adjectives_df = pd.DataFrame(most_common_adjectives, columns=['Word', 'Frequency'])\n",
    "print(most_common_adjectives_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word  Frequency\n",
      "0       more         19\n",
      "1       most          8\n",
      "2     higher          5\n",
      "3    biggest          4\n",
      "4       less          3\n",
      "5       best          3\n",
      "6    cheaper          3\n",
      "7      lower          3\n",
      "8      fewer          2\n",
      "9    nigeria          2\n",
      "10    better          2\n",
      "11    bigger          1\n",
      "12  greatest          1\n",
      "13   speaker          1\n",
      "14    hunter          1\n",
      "15     worse          1\n",
      "16     older          1\n",
      "17     least          1\n",
      "18     saudi          1\n",
      "19   lighter          1\n",
      "20   fastest          1\n",
      "21  wildfire          1\n",
      "22   greater          1\n",
      "23    matter          1\n",
      "24   highest          1\n",
      "25   richest          1\n",
      "                                               claim    label  \\\n",
      "0  In a letter to Steve Jobs, Sean Connery refuse...  Refuted   \n",
      "1  Trump Administration claimed songwriter Billie...  Refuted   \n",
      "2  Due to Imran Khan's criticism of Macron's comm...  Refuted   \n",
      "3  UNESCO declared Nadar community as the most an...  Refuted   \n",
      "4  Republican Matt Gaetz was part of a company th...  Refuted   \n",
      "\n",
      "   common_adjective_count  \n",
      "0                       0  \n",
      "1                       0  \n",
      "2                       0  \n",
      "3                       1  \n",
      "4                       0  \n",
      "label\n",
      "Conflicting Evidence/Cherrypicking    0.157895\n",
      "Not Enough Evidence                   0.885714\n",
      "Refuted                               0.157377\n",
      "Supported                             0.352459\n",
      "Name: common_adjective_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'my_env/data/dev.json'  # Change to your path\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "# Preprocess the text to make it easier to extract words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "data['tokens'] = data['claim'].apply(preprocess_text)\n",
    "\n",
    "# Flatten all tokens into a single list\n",
    "all_tokens = [token for tokens in data['tokens'] for token in tokens]\n",
    "\n",
    "# POS tagging to filter adjectives and emotional words\n",
    "tagged_tokens = nltk.pos_tag(all_tokens)\n",
    "adjective_words = [word for word, pos in tagged_tokens if pos in ['JJR', 'JJS', 'RBR', 'RBS']]\n",
    "\n",
    "# Frequency distribution of adjectives/emotional words\n",
    "freq_dist = Counter(adjective_words)\n",
    "\n",
    "# Get the 100 most common adjectives/emotional words\n",
    "most_common_adjectives = freq_dist.most_common(100)\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "most_common_adjectives_df = pd.DataFrame(most_common_adjectives, columns=['Word', 'Frequency'])\n",
    "print(most_common_adjectives_df)\n",
    "\n",
    "# Create a set of the most common adjectives for quick lookup\n",
    "most_common_adjectives_set = set([word for word, _ in most_common_adjectives])\n",
    "\n",
    "# Function to count the number of most common adjectives in each claim\n",
    "def count_common_adjectives(tokens, common_adjectives):\n",
    "    count = sum(1 for word in tokens if word in common_adjectives)\n",
    "    return count\n",
    "\n",
    "# Apply preprocessing and count common adjectives\n",
    "data['common_adjective_count'] = data['tokens'].apply(lambda tokens: count_common_adjectives(tokens, most_common_adjectives_set))\n",
    "\n",
    "# Display the DataFrame with the new columns\n",
    "print(data[['claim', 'label', 'common_adjective_count']].head())\n",
    "\n",
    "# Analyze the correlation between common adjective count and the labels\n",
    "common_adjective_count_analysis = data.groupby('label')['common_adjective_count'].mean()\n",
    "print(common_adjective_count_analysis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
