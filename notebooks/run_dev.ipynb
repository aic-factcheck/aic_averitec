{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run pipeline (or its steps) on dev data and obtain METEOR-0.25 score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "!python -m spacy download en_core_web_lg\n",
    "!python -m nltk.downloader punkt\n",
    "!python -m nltk.downloader wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 0. Prep environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "!python -m spacy download en_core_web_lg\n",
    "!python -m nltk.downloader punkt\n",
    "!python -m nltk.downloader wordnet\n",
    "!sh script/copy_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé 1. Get precomputed Google API results and Scrape text from their URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash script/scraper.sh dev 0 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•á 2. Rank Search results with BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/reranking/bm25_sentences.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨  3. Generate QA pair for the top sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/reranking/question_generation_top_sentences.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•à 4. Rerank the QA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/reranking/rerank_questions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§• 5. Predict veracity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/prediction/veracity_prediction.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 6. Evaluate the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question-only score (HU-meteor):             0.24041210604919014\n",
      "Question-answer score (HU-meteor):           0.18547341231661782\n",
      "====================\n",
      "Veracity F1 scores:\n",
      " * Supported:                                0.4372093023255814\n",
      " * Refuted:                                  0.7138157894736842\n",
      " * Not Enough Evidence:                      0.0\n",
      " * Conflicting Evidence/Cherrypicking:       0.13333333333333333\n",
      " * macro:                                    0.32108960628314975\n",
      " * acc:                                      0.546\n",
      "--------------------\n",
      "AVeriTeC scores:\n",
      " * Veracity scores (meteor @ 0.1):           0.452\n",
      " * Veracity scores (meteor @ 0.2):           0.186\n",
      " * Veracity scores (meteor @ 0.25):          0.092\n",
      " * Veracity scores (meteor @ 0.3):           0.05\n",
      " * Veracity scores (meteor @ 0.4):           0.012\n",
      " * Veracity scores (meteor @ 0.5):           0.002\n",
      "--------------------\n",
      "AVeriTeC scores by type @ 0.25:\n",
      " * Veracity scores (Event/Property Claim):   0.05979024836242316\n",
      " * Veracity scores (Position Statement):     0.07307235683162566\n",
      " * Veracity scores (Causal Claim):           0.056993318700669676\n",
      " * Veracity scores (Numerical Claim):        0.070268725336461\n",
      " * Veracity scores (Quote Verification):     0.06266365543813311\n"
     ]
    }
   ],
   "source": [
    "%run src/prediction/evaluate_veracity.py --label_file data/dev.json --prediction_file data_store/dev_veracity_prediction_bck.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question-only score (HU-meteor):             0.2647792777614756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question-answer score (HU-meteor):           0.17693171723529702\n",
      "====================\n",
      "Veracity F1 scores:\n",
      " * Supported:                                0.5714285714285714\n",
      " * Refuted:                                  0.8\n",
      " * Not Enough Evidence:                      0.0\n",
      " * Conflicting Evidence/Cherrypicking:       0.0\n",
      " * macro:                                    0.34285714285714286\n",
      " * acc:                                      0.6\n",
      "--------------------\n",
      "AVeriTeC scores:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ullriher/venvs/averitec/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ullriher/venvs/averitec/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Veracity scores (meteor @ 0.1):           0.6\n",
      " * Veracity scores (meteor @ 0.2):           0.1\n",
      " * Veracity scores (meteor @ 0.25):          0.0\n",
      " * Veracity scores (meteor @ 0.3):           0.0\n",
      " * Veracity scores (meteor @ 0.4):           0.0\n",
      " * Veracity scores (meteor @ 0.5):           0.0\n",
      "--------------------\n",
      "AVeriTeC scores by type @ 0.25:\n",
      " * Veracity scores (Position Statement):     0.13407472728460382\n",
      " * Veracity scores (Quote Verification):     0.0\n",
      " * Veracity scores (Event/Property Claim):   0.0\n",
      " * Veracity scores (Causal Claim):           0.0\n",
      " * Veracity scores (Numerical Claim):        0.0\n"
     ]
    }
   ],
   "source": [
    "%run src/prediction/evaluate_veracity.py --label_file data/dev10.json --prediction_file data_store/dev10_veracity_prediction.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question-only score (HU-meteor):             0.4456959937506465\n",
      "Question-answer score (HU-meteor):           0.305419432129659\n",
      "====================\n",
      "Veracity F1 scores:\n",
      " * Supported:                                0.7272727272727273\n",
      " * Refuted:                                  0.9090909090909091\n",
      " * Not Enough Evidence:                      0.0\n",
      " * Conflicting Evidence/Cherrypicking:       0.0\n",
      " * macro:                                    0.40909090909090906\n",
      " * acc:                                      0.7\n",
      "--------------------\n",
      "AVeriTeC scores:\n",
      " * Veracity scores (meteor @ 0.1):           0.7\n",
      " * Veracity scores (meteor @ 0.2):           0.55\n",
      " * Veracity scores (meteor @ 0.25):          0.4\n",
      " * Veracity scores (meteor @ 0.3):           0.3\n",
      " * Veracity scores (meteor @ 0.4):           0.15\n",
      " * Veracity scores (meteor @ 0.5):           0.05\n",
      "--------------------\n",
      "AVeriTeC scores by type @ 0.25:\n",
      " * Veracity scores (Event/Property Claim):   0.27871912377066566\n",
      " * Veracity scores (Position Statement):     0.27455467133913136\n",
      " * Veracity scores (Quote Verification):     0.0\n",
      " * Veracity scores (Causal Claim):           0.0\n",
      " * Veracity scores (Numerical Claim):        0.1850511281986735\n"
     ]
    }
   ],
   "source": [
    "%run src/prediction/evaluate_veracity.py --label_file data/dev_20.json --prediction_file data_store/dev_20_claude.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
