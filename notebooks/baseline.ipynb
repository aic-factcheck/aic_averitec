{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVeriTeC Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r AVeriTeC/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Search for Evidence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scrape text from the URLs obtained by searching queries with the Google API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rank the sentences in the knowledge store with BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import nltk\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "\n",
    "def combine_all_sentences(knowledge_file):\n",
    "    sentences, urls = [], []\n",
    "\n",
    "    with open(knowledge_file, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        for i, line in enumerate(json_file):\n",
    "            data = json.loads(line)\n",
    "            sentences.extend(data[\"url2text\"])\n",
    "            urls.extend([data[\"url\"] for i in range(len(data[\"url2text\"]))])\n",
    "    return sentences, urls, i + 1\n",
    "\n",
    "\n",
    "def retrieve_top_k_sentences(query, document, urls, top_k):\n",
    "    tokenized_docs = [nltk.word_tokenize(doc) for doc in document]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    scores = bm25.get_scores(nltk.word_tokenize(query))\n",
    "    top_k_idx = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "    return [document[i] for i in top_k_idx], [urls[i] for i in top_k_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "Processing claim 0... Progress: 1 / 500\n",
      "Obtained 534489 sentences from 825 urls.\n",
      "Top 100 retrieved. Time elapsed: 67.17283320426941.\n",
      "Processing claim 1... Progress: 2 / 500\n",
      "Obtained 344674 sentences from 714 urls.\n",
      "Top 100 retrieved. Time elapsed: 42.56042766571045.\n",
      "Processing claim 2... Progress: 3 / 500\n",
      "Obtained 1182335 sentences from 1300 urls.\n",
      "Top 100 retrieved. Time elapsed: 151.2538583278656.\n",
      "Processing claim 3... Progress: 4 / 500\n",
      "Obtained 1104825 sentences from 1041 urls.\n",
      "Top 100 retrieved. Time elapsed: 136.4850766658783.\n",
      "Processing claim 4... Progress: 5 / 500\n",
      "Obtained 552547 sentences from 1064 urls.\n",
      "Top 100 retrieved. Time elapsed: 69.11488580703735.\n",
      "Processing claim 5... Progress: 6 / 500\n",
      "Obtained 960056 sentences from 822 urls.\n",
      "Top 100 retrieved. Time elapsed: 116.33663892745972.\n",
      "Processing claim 6... Progress: 7 / 500\n",
      "Obtained 723400 sentences from 1392 urls.\n",
      "Top 100 retrieved. Time elapsed: 97.06489825248718.\n",
      "Processing claim 7... Progress: 8 / 500\n",
      "Obtained 729765 sentences from 1092 urls.\n",
      "Top 100 retrieved. Time elapsed: 99.47640013694763.\n",
      "Processing claim 8... Progress: 9 / 500\n",
      "Obtained 396646 sentences from 1026 urls.\n",
      "Top 100 retrieved. Time elapsed: 53.063013315200806.\n",
      "Processing claim 9... Progress: 10 / 500\n",
      "Obtained 641512 sentences from 963 urls.\n",
      "Top 100 retrieved. Time elapsed: 93.52473044395447.\n",
      "Processing claim 10... Progress: 11 / 500\n",
      "Obtained 894520 sentences from 1094 urls.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Retrieve top_k sentences with bm25\u001b[39;00m\n\u001b[1;32m     36\u001b[0m st \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 37\u001b[0m top_k_sentences, top_k_urls \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_top_k_sentences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclaim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument_in_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTOP_K\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTOP_K\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m retrieved. Time elapsed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mst\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m json_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaim_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: idx,\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaim\u001b[39m\u001b[38;5;124m\"\u001b[39m: example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaim\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     ],\n\u001b[1;32m     49\u001b[0m }\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mretrieve_top_k_sentences\u001b[0;34m(query, document, urls, top_k)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_top_k_sentences\u001b[39m(query, document, urls, top_k):\n\u001b[0;32m---> 22\u001b[0m     tokenized_docs \u001b[38;5;241m=\u001b[39m [nltk\u001b[38;5;241m.\u001b[39mword_tokenize(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m document]\n\u001b[1;32m     23\u001b[0m     bm25 \u001b[38;5;241m=\u001b[39m BM25Okapi(tokenized_docs)\n\u001b[1;32m     24\u001b[0m     scores \u001b[38;5;241m=\u001b[39m bm25\u001b[38;5;241m.\u001b[39mget_scores(nltk\u001b[38;5;241m.\u001b[39mword_tokenize(query))\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_top_k_sentences\u001b[39m(query, document, urls, top_k):\n\u001b[0;32m---> 22\u001b[0m     tokenized_docs \u001b[38;5;241m=\u001b[39m [\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m document]\n\u001b[1;32m     23\u001b[0m     bm25 \u001b[38;5;241m=\u001b[39m BM25Okapi(tokenized_docs)\n\u001b[1;32m     24\u001b[0m     scores \u001b[38;5;241m=\u001b[39m bm25\u001b[38;5;241m.\u001b[39mget_scores(nltk\u001b[38;5;241m.\u001b[39mword_tokenize(query))\n",
      "File \u001b[0;32m~/venvs/py3.10.4/lib/python3.10/site-packages/nltk/tokenize/__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/venvs/py3.10.4/lib/python3.10/site-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/venvs/py3.10.4/lib/python3.10/site-packages/nltk/tokenize/destructive.py:160\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    157\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPUNCTUATION:\n\u001b[0;32m--> 160\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstitution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Handles parentheses.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m regexp, substitution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPARENS_BRACKETS\n",
      "File \u001b[0;32m/mnt/appl/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/re.py:330\u001b[0m, in \u001b[0;36m_subx.<locals>.filter\u001b[0;34m(match, template)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m template[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(template[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;66;03m# literal replacement\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m template[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter\u001b[39m(match, template\u001b[38;5;241m=\u001b[39mtemplate):\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sre_parse\u001b[38;5;241m.\u001b[39mexpand_template(template, match)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "KNOWLEDGE_STORE_DIR = \"AVeriTeC/data_store/output_dev\" # The path of the knowledge_store_dir containing json files with all the retrieved sentences.\n",
    "CLAIM_FILE = \"AVeriTeC/data/dev.json\" # The path of the file that stores the claim.\n",
    "JSON_OUTPUT = \"AVeriTeC/data_store/dev_top_k.json\" # The output dir for JSON files to save the top 100 sentences for each claim.\n",
    "TOP_K = 100 # How many documents should we pick out with BM25.\n",
    "START = 0 # Staring index of the files to process.\n",
    "END = -1 # End index of the files to process.\n",
    "    \n",
    "with open(CLAIM_FILE, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    target_examples = json.load(json_file)\n",
    "    \n",
    "    #set end to number of files in the directory knowledge_store_dir\n",
    "    if END == -1:\n",
    "        END = len(os.listdir(KNOWLEDGE_STORE_DIR))\n",
    "        print(END)\n",
    "\n",
    "    files_to_process = list(range(START, END))\n",
    "    total = len(files_to_process)\n",
    "\n",
    "    with open(JSON_OUTPUT, \"w\", encoding=\"utf-8\") as output_json:\n",
    "        done = 0\n",
    "        for idx, example in enumerate(target_examples):\n",
    "            # Load the knowledge store for this example\n",
    "            if idx in files_to_process:\n",
    "                print(f\"Processing claim {idx}... Progress: {done + 1} / {total}\")\n",
    "                document_in_sentences, sentence_urls, num_urls_this_claim = (\n",
    "                    combine_all_sentences(\n",
    "                        os.path.join(KNOWLEDGE_STORE_DIR, f\"{idx}.json\")\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"Obtained {len(document_in_sentences)} sentences from {num_urls_this_claim} urls.\"\n",
    "                )\n",
    "\n",
    "                # Retrieve top_k sentences with bm25\n",
    "                st = time.time()\n",
    "                top_k_sentences, top_k_urls = retrieve_top_k_sentences(\n",
    "                    example[\"claim\"], document_in_sentences, sentence_urls, TOP_K\n",
    "                )\n",
    "                print(f\"Top {TOP_K} retrieved. Time elapsed: {time.time() - st}.\")\n",
    "\n",
    "                json_data = {\n",
    "                    \"claim_id\": idx,\n",
    "                    \"claim\": example[\"claim\"],\n",
    "                    f\"top_{TOP_K}\": [\n",
    "                        {\"sentence\": sent, \"url\": url}\n",
    "                        for sent, url in zip(top_k_sentences, top_k_urls)\n",
    "                    ],\n",
    "                }\n",
    "                output_json.write(json.dumps(json_data, ensure_ascii=False) + \"\\n\")\n",
    "                done += 1\n",
    "                # output_file.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate questions-answer pair for the top sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import nltk\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BloomTokenizerFast, BloomForCausalLM\n",
    "\n",
    "\n",
    "def claim2prompts(example):\n",
    "    claim = example[\"claim\"]\n",
    "\n",
    "    # claim_str = \"Claim: \" + claim + \"||Evidence: \"\n",
    "    claim_str = \"Evidence: \"\n",
    "\n",
    "    for question in example[\"questions\"]:\n",
    "        q_text = question[\"question\"].strip()\n",
    "        if len(q_text) == 0:\n",
    "            continue\n",
    "\n",
    "        if not q_text[-1] == \"?\":\n",
    "            q_text += \"?\"\n",
    "\n",
    "        answer_strings = []\n",
    "\n",
    "        for a in question[\"answers\"]:\n",
    "            if a[\"answer_type\"] in [\"Extractive\", \"Abstractive\"]:\n",
    "                answer_strings.append(a[\"answer\"])\n",
    "            if a[\"answer_type\"] == \"Boolean\":\n",
    "                answer_strings.append(\n",
    "                    a[\"answer\"]\n",
    "                    + \", because \"\n",
    "                    + a[\"boolean_explanation\"].lower().strip()\n",
    "                )\n",
    "\n",
    "        for a_text in answer_strings:\n",
    "            if not a_text[-1] in [\".\", \"!\", \":\", \"?\"]:\n",
    "                a_text += \".\"\n",
    "\n",
    "            # prompt_lookup_str = claim + \" \" + a_text\n",
    "            prompt_lookup_str = a_text\n",
    "            this_q_claim_str = (\n",
    "                claim_str + \" \" + a_text.strip() + \"||Question answered: \" + q_text\n",
    "            )\n",
    "            yield (\n",
    "                prompt_lookup_str,\n",
    "                this_q_claim_str.replace(\"\\n\", \" \").replace(\"||\", \"\\n\"),\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE_CORPUS = \"AVeriTeC/data/train.json\" # The path of the reference corpus.\n",
    "TARGET_FILE = \"AVeriTeC/data/dev.json\" # The path of the file that stores the claim.\n",
    "TOP_K_TARGET_KNOWLEDGE = \"data_store/dev_top_k_sentences.json\" #\"Directory where the sentences for the scraped data is saved.\"\n",
    "OUTPUT_QUESTIONS = \"data_store/dev_top_k_qa.json\" # Directory where the sentences for the scraped data is saved.\n",
    "TOP_K = 100 # How many documents should we pick out with BM25.\n",
    "\n",
    "# few-shot learning from the training set\n",
    "with open(REFERENCE_CORPUS, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    train_examples = json.load(json_file)\n",
    "\n",
    "prompt_corpus, tokenized_corpus = [], []\n",
    "\n",
    "for example in train_examples:\n",
    "    for lookup_str, prompt in claim2prompts(example):\n",
    "        entry = nltk.word_tokenize(lookup_str)\n",
    "        tokenized_corpus.append(entry)\n",
    "        prompt_corpus.append(prompt)\n",
    "\n",
    "prompt_bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Load the bloom model:\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-7b1\")\n",
    "model = BloomForCausalLM.from_pretrained(\n",
    "    \"bigscience/bloom-7b1\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    offload_folder=\"./offload\",\n",
    ")\n",
    "\n",
    "with open(OUTPUT_QUESTIONS, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    with open(TOP_K_TARGET_KNOWLEDGE, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        for i, line in enumerate(json_file):\n",
    "            data = json.loads(line)\n",
    "            top_k_sentences_urls = data[f\"top_{TOP_K}\"]\n",
    "            claim = data[\"claim\"]\n",
    "            claim_id = data[\"claim_id\"]\n",
    "\n",
    "            bm25_qau = []  # question, answer, url\n",
    "            # Generate questions for those top k:\n",
    "            for sent_i, sentences_urls in enumerate(top_k_sentences_urls):\n",
    "\n",
    "                prompt_lookup_str = sentences_urls[\"sentence\"]\n",
    "                url = sentences_urls[\"url\"]\n",
    "                prompt_s = prompt_bm25.get_scores(nltk.word_tokenize(prompt_lookup_str))\n",
    "                prompt_n = 10\n",
    "                prompt_top_n = np.argsort(prompt_s)[::-1][:prompt_n]\n",
    "                prompt_docs = [prompt_corpus[i] for i in prompt_top_n]\n",
    "\n",
    "                claim_prompt = (\n",
    "                    \"Evidence: \"\n",
    "                    + prompt_lookup_str.replace(\"\\n\", \" \")\n",
    "                    + \"\\nQuestion answered: \"\n",
    "                )\n",
    "\n",
    "                prompt = \"\\n\\n\".join(prompt_docs + [claim_prompt])\n",
    "\n",
    "                inputs = tokenizer([prompt], padding=True, return_tensors=\"pt\").to(\n",
    "                    model.device\n",
    "                )\n",
    "                st = time.time()\n",
    "                outputs = model.generate(\n",
    "                    inputs[\"input_ids\"],\n",
    "                    max_length=5000,\n",
    "                    num_beams=2,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                    early_stopping=True,\n",
    "                )\n",
    "                print(f\"Generated QA for sent {sent_i} in file {i}. Time elapsed: {time.time() - st}\")\n",
    "\n",
    "                tgt_text = tokenizer.batch_decode(\n",
    "                    outputs[:, inputs[\"input_ids\"].shape[-1] :],\n",
    "                    skip_special_tokens=True,\n",
    "                )[0]\n",
    "\n",
    "                # We are not allowed to generate more than 250 characters:\n",
    "                tgt_text = tgt_text[:250]\n",
    "\n",
    "                qau_pair = [\n",
    "                    tgt_text.strip().split(\"?\")[0].replace(\"\\n\", \" \") + \"?\",\n",
    "                    prompt_lookup_str.replace(\"\\n\", \" \"),\n",
    "                    url,\n",
    "                ]\n",
    "\n",
    "                bm25_qau.append(qau_pair)\n",
    "\n",
    "            json_data = {\n",
    "                \"claim_id\": claim_id,\n",
    "                \"claim\": claim,\n",
    "                \"bm25_qau\": bm25_qau,\n",
    "            }\n",
    "            output_file.write(json.dumps(json_data, ensure_ascii=False) + \"\\n\")\n",
    "            output_file.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rerank the QA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from src.models.DualEncoderModule import DualEncoderModule\n",
    "\n",
    "\n",
    "def triple_to_string(x):\n",
    "    return \" </s> \".join([item.strip() for item in x])\n",
    "\n",
    "TOP_K_QA_FILE = \"data_store/dev_top_k_qa.json\" #Json file with claim and top k generated question-answer pairs.\n",
    "OUTPUT_FILE = \"data_store/dev_top_3_rerank_qa.json\" #Json file with the top3 reranked questions.\n",
    "BEST_CHECKPOINT = \"pretrained_models/bert_dual_encoder.ckpt\"\n",
    "TOP_N = 3 #top_n question answer pairs as evidence to keep.\n",
    "\n",
    "examples = []\n",
    "with open(TOP_K_QA_FILE) as f:\n",
    "    for line in f:\n",
    "        examples.append(json.loads(line))\n",
    "\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    bert_model_name, num_labels=2, problem_type=\"single_label_classification\"\n",
    ")\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "trained_model = DualEncoderModule.load_from_checkpoint(\n",
    "    BEST_CHECKPOINT, tokenizer=tokenizer, model=bert_model\n",
    ").to(device)\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for example in tqdm.tqdm(examples):\n",
    "        strs_to_score = []\n",
    "        values = []\n",
    "\n",
    "        bm25_qau = example[\"bm25_qau\"] if \"bm25_qau\" in example else []\n",
    "        claim = example[\"claim\"]\n",
    "\n",
    "        for question, answer, url in bm25_qau:\n",
    "            str_to_score = triple_to_string([claim, question, answer])\n",
    "\n",
    "            strs_to_score.append(str_to_score)\n",
    "            values.append([question, answer, url])\n",
    "\n",
    "        if len(bm25_qau) > 0:\n",
    "            encoded_dict = tokenizer(\n",
    "                strs_to_score,\n",
    "                max_length=512,\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "\n",
    "            input_ids = encoded_dict[\"input_ids\"]\n",
    "            attention_masks = encoded_dict[\"attention_mask\"]\n",
    "\n",
    "            scores = torch.softmax(\n",
    "                trained_model(input_ids, attention_mask=attention_masks).logits,\n",
    "                axis=-1,\n",
    "            )[:, 1]\n",
    "\n",
    "            top_n = torch.argsort(scores, descending=True)[: TOP_N]\n",
    "            evidence = [\n",
    "                {\n",
    "                    \"question\": values[i][0],\n",
    "                    \"answer\": values[i][1],\n",
    "                    \"url\": values[i][2],\n",
    "                }\n",
    "                for i in top_n\n",
    "            ]\n",
    "        else:\n",
    "            evidence = []\n",
    "\n",
    "        json_data = {\n",
    "            \"claim_id\": example[\"claim_id\"],\n",
    "            \"claim\": claim,\n",
    "            \"evidence\": evidence,\n",
    "        }\n",
    "        output_file.write(json.dumps(json_data, ensure_ascii=False) + \"\\n\")\n",
    "        output_file.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Veracity prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import tqdm\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from src.models.SequenceClassificationModule import SequenceClassificationModule\n",
    "\n",
    "\n",
    "LABEL = [\n",
    "    \"Supported\",\n",
    "    \"Refuted\",\n",
    "    \"Not Enough Evidence\",\n",
    "    \"Conflicting Evidence/Cherrypicking\",\n",
    "]\n",
    "\n",
    "\n",
    "class SequenceClassificationDataLoader(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer, data_file, batch_size, add_extra_nee=False):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_file = data_file\n",
    "        self.batch_size = batch_size\n",
    "        self.add_extra_nee = add_extra_nee\n",
    "\n",
    "    def tokenize_strings(\n",
    "        self,\n",
    "        source_sentences,\n",
    "        max_length=400,\n",
    "        pad_to_max_length=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ):\n",
    "        encoded_dict = self.tokenizer(\n",
    "            source_sentences,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\" if pad_to_max_length else \"longest\",\n",
    "            truncation=True,\n",
    "            return_tensors=return_tensors,\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_dict[\"input_ids\"]\n",
    "        attention_masks = encoded_dict[\"attention_mask\"]\n",
    "\n",
    "        return input_ids, attention_masks\n",
    "\n",
    "    def quadruple_to_string(self, claim, question, answer, bool_explanation=\"\"):\n",
    "        if bool_explanation is not None and len(bool_explanation) > 0:\n",
    "            bool_explanation = \", because \" + bool_explanation.lower().strip()\n",
    "        else:\n",
    "            bool_explanation = \"\"\n",
    "        return (\n",
    "            \"[CLAIM] \"\n",
    "            + claim.strip()\n",
    "            + \" [QUESTION] \"\n",
    "            + question.strip()\n",
    "            + \" \"\n",
    "            + answer.strip()\n",
    "            + bool_explanation\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAIM_WITH_EVIDENCE_FILE = \"data_store/dev_top_3_rerank_qa.json\" #Json file with claim and top question-answer pairs as evidence.\n",
    "OUTPUT_FILE = \"data_store/dev_veracity_prediction.json\" #Json file with the veracity predictions.\n",
    "BEST_CHECKPOINT = \"pretrained_models/bert_veracity.ckpt\"\n",
    "\n",
    "examples = []\n",
    "with open(args.claim_with_evidence_file) as f:\n",
    "    for line in f:\n",
    "        examples.append(json.loads(line))\n",
    "\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    bert_model_name, num_labels=4, problem_type=\"single_label_classification\"\n",
    ")\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "trained_model = SequenceClassificationModule.load_from_checkpoint(\n",
    "    args.best_checkpoint, tokenizer=tokenizer, model=bert_model\n",
    ").to(device)\n",
    "\n",
    "dataLoader = SequenceClassificationDataLoader(\n",
    "    tokenizer=tokenizer,\n",
    "    data_file=\"this_is_discontinued\",\n",
    "    batch_size=32,\n",
    "    add_extra_nee=False,\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for example in tqdm.tqdm(examples):\n",
    "    example_strings = []\n",
    "    for evidence in example[\"evidence\"]:\n",
    "        example_strings.append(\n",
    "            dataLoader.quadruple_to_string(\n",
    "                example[\"claim\"], evidence[\"question\"], evidence[\"answer\"], \"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if (\n",
    "        len(example_strings) == 0\n",
    "    ):  # If we found no evidence e.g. because google returned 0 pages, just output NEI.\n",
    "            example[\"label\"] = \"Not Enough Evidence\"\n",
    "            continue\n",
    "\n",
    "    tokenized_strings, attention_mask = dataLoader.tokenize_strings(example_strings)\n",
    "    example_support = torch.argmax(\n",
    "        trained_model(\n",
    "            tokenized_strings.to(device), attention_mask=attention_mask.to(device)\n",
    "        ).logits,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    has_unanswerable = False\n",
    "    has_true = False\n",
    "    has_false = False\n",
    "\n",
    "    for v in example_support:\n",
    "        if v == 0:\n",
    "            has_true = True\n",
    "        if v == 1:\n",
    "            has_false = True\n",
    "        if v in (\n",
    "            2,\n",
    "            3,\n",
    "        ):  # TODO another hack -- we cant have different labels for train and test so we do this\n",
    "            has_unanswerable = True\n",
    "\n",
    "    if has_unanswerable:\n",
    "        answer = 2\n",
    "    elif has_true and not has_false:\n",
    "        answer = 0\n",
    "    elif not has_true and has_false:\n",
    "        answer = 1\n",
    "    else:\n",
    "        answer = 3\n",
    "\n",
    "    json_data = {\n",
    "        \"claim_id\": example[\"claim_id\"],\n",
    "        \"claim\": example[\"claim\"],\n",
    "        \"evidence\": example[\"evidence\"],\n",
    "        \"pred_label\": LABEL[answer],\n",
    "    }\n",
    "    predictions.append(json_data)\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    json.dump(predictions, output_file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
